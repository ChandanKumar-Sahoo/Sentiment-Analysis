{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xT7MKZuMRaCg"
   },
   "source": [
    "# Sentiment Classification\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "NSyEcWKegYvg"
   },
   "outputs": [],
   "source": [
    "# Let's import some of the packages to perform classification task\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Embedding, LSTM, SpatialDropout1D\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.utils.np_utils import to_categorical\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Wq4RCyyPSYRp"
   },
   "source": [
    "## Loading the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "NGCtiXUhSWss",
    "outputId": "7c47a62b-49e5-4f14-ece1-c9cbce2c0f0f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/imdb.npz\n",
      "17465344/17464789 [==============================] - 0s 0us/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<string>:6: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/datasets/imdb.py:159: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  x_train, y_train = np.array(xs[:idx]), np.array(labels[:idx])\n",
      "/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/datasets/imdb.py:160: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  x_test, y_test = np.array(xs[idx:]), np.array(labels[idx:])\n"
     ]
    }
   ],
   "source": [
    "from keras.datasets import imdb\n",
    "\n",
    "vocab_size = 10000 #vocab size\n",
    "INDEX_FROM=3  # word index offset\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=vocab_size, index_from=INDEX_FROM) # vocab_size is no.of words to consider from the dataset, ordering based on frequency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vJO5pW2DDwF4",
    "outputId": "4a626a70-3924-4328-915a-7c1d1fc4b722"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Types of catagories: [0 1]\n",
      "Number of unique words: 49579\n"
     ]
    }
   ],
   "source": [
    "# Concatinating both train and test data to get into the the dataset\n",
    "# It is to be noted that, both train and test set shape is same.\n",
    "\n",
    "features = np.concatenate((x_train, x_test), axis=0)\n",
    "target = np.concatenate((y_train, y_test), axis=0)\n",
    "\n",
    "print(\"Types of catagories:\", np.unique(target))\n",
    "print(\"Number of unique words:\", len(np.unique(features)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2jq6ERPkpCob"
   },
   "source": [
    "- The dataset has 49579 words in total and has two class target value. \n",
    "- 0 stands for negative review and 1 stands for positive review."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "fCPC_WN-eCyw"
   },
   "outputs": [],
   "source": [
    "from keras.preprocessing.sequence import pad_sequences\n",
    "vocab_size = 10000 #vocab size\n",
    "maxlen = 300  #number of word used from each review"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qMEsHYrWxdtk"
   },
   "source": [
    "## Train test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "h0g381XzeCyz",
    "outputId": "61970068-1134-493b-de4a-e08b7eb60299"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<string>:6: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/datasets/imdb.py:159: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  x_train, y_train = np.array(xs[:idx]), np.array(labels[:idx])\n",
      "/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/datasets/imdb.py:160: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  x_test, y_test = np.array(xs[idx:]), np.array(labels[idx:])\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "#load dataset as a list of ints\n",
    "(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=vocab_size)\n",
    "#make all sequences of the same length\n",
    "x_train = pad_sequences(x_train, maxlen=maxlen)\n",
    "x_test =  pad_sequences(x_test, maxlen=maxlen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Jy6n-uM2eCy2",
    "outputId": "67cec399-affd-4477-d222-49e9f23116aa"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(25000, 300) (25000,)\n"
     ]
    }
   ],
   "source": [
    "print(x_train.shape, y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "tZhMAgaNeCy5",
    "outputId": "9929e93f-635a-4064-fcce-77b607d9c60d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(25000, 300) (25000,)\n"
     ]
    }
   ],
   "source": [
    "print(x_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SzCwtasAmRaF"
   },
   "source": [
    "- Here we've not considered any specific split ratio, and continued with the given test and train data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 253
    },
    "id": "UIkyBHG7nkYv",
    "outputId": "26b20643-5bc7-4287-e9d3-b6bb9b816e0f"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "      <th>15</th>\n",
       "      <th>16</th>\n",
       "      <th>17</th>\n",
       "      <th>18</th>\n",
       "      <th>19</th>\n",
       "      <th>20</th>\n",
       "      <th>21</th>\n",
       "      <th>22</th>\n",
       "      <th>23</th>\n",
       "      <th>24</th>\n",
       "      <th>25</th>\n",
       "      <th>26</th>\n",
       "      <th>27</th>\n",
       "      <th>28</th>\n",
       "      <th>29</th>\n",
       "      <th>30</th>\n",
       "      <th>31</th>\n",
       "      <th>32</th>\n",
       "      <th>33</th>\n",
       "      <th>34</th>\n",
       "      <th>35</th>\n",
       "      <th>36</th>\n",
       "      <th>37</th>\n",
       "      <th>38</th>\n",
       "      <th>39</th>\n",
       "      <th>...</th>\n",
       "      <th>260</th>\n",
       "      <th>261</th>\n",
       "      <th>262</th>\n",
       "      <th>263</th>\n",
       "      <th>264</th>\n",
       "      <th>265</th>\n",
       "      <th>266</th>\n",
       "      <th>267</th>\n",
       "      <th>268</th>\n",
       "      <th>269</th>\n",
       "      <th>270</th>\n",
       "      <th>271</th>\n",
       "      <th>272</th>\n",
       "      <th>273</th>\n",
       "      <th>274</th>\n",
       "      <th>275</th>\n",
       "      <th>276</th>\n",
       "      <th>277</th>\n",
       "      <th>278</th>\n",
       "      <th>279</th>\n",
       "      <th>280</th>\n",
       "      <th>281</th>\n",
       "      <th>282</th>\n",
       "      <th>283</th>\n",
       "      <th>284</th>\n",
       "      <th>285</th>\n",
       "      <th>286</th>\n",
       "      <th>287</th>\n",
       "      <th>288</th>\n",
       "      <th>289</th>\n",
       "      <th>290</th>\n",
       "      <th>291</th>\n",
       "      <th>292</th>\n",
       "      <th>293</th>\n",
       "      <th>294</th>\n",
       "      <th>295</th>\n",
       "      <th>296</th>\n",
       "      <th>297</th>\n",
       "      <th>298</th>\n",
       "      <th>299</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>22</td>\n",
       "      <td>21</td>\n",
       "      <td>134</td>\n",
       "      <td>476</td>\n",
       "      <td>26</td>\n",
       "      <td>480</td>\n",
       "      <td>5</td>\n",
       "      <td>144</td>\n",
       "      <td>30</td>\n",
       "      <td>5535</td>\n",
       "      <td>18</td>\n",
       "      <td>51</td>\n",
       "      <td>36</td>\n",
       "      <td>28</td>\n",
       "      <td>224</td>\n",
       "      <td>92</td>\n",
       "      <td>25</td>\n",
       "      <td>104</td>\n",
       "      <td>4</td>\n",
       "      <td>226</td>\n",
       "      <td>65</td>\n",
       "      <td>16</td>\n",
       "      <td>38</td>\n",
       "      <td>1334</td>\n",
       "      <td>88</td>\n",
       "      <td>12</td>\n",
       "      <td>16</td>\n",
       "      <td>283</td>\n",
       "      <td>5</td>\n",
       "      <td>16</td>\n",
       "      <td>4472</td>\n",
       "      <td>113</td>\n",
       "      <td>103</td>\n",
       "      <td>32</td>\n",
       "      <td>15</td>\n",
       "      <td>16</td>\n",
       "      <td>5345</td>\n",
       "      <td>19</td>\n",
       "      <td>178</td>\n",
       "      <td>32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>152</td>\n",
       "      <td>491</td>\n",
       "      <td>18</td>\n",
       "      <td>2</td>\n",
       "      <td>32</td>\n",
       "      <td>7464</td>\n",
       "      <td>1212</td>\n",
       "      <td>14</td>\n",
       "      <td>9</td>\n",
       "      <td>6</td>\n",
       "      <td>371</td>\n",
       "      <td>78</td>\n",
       "      <td>22</td>\n",
       "      <td>625</td>\n",
       "      <td>64</td>\n",
       "      <td>1382</td>\n",
       "      <td>9</td>\n",
       "      <td>8</td>\n",
       "      <td>168</td>\n",
       "      <td>145</td>\n",
       "      <td>23</td>\n",
       "      <td>4</td>\n",
       "      <td>1690</td>\n",
       "      <td>15</td>\n",
       "      <td>16</td>\n",
       "      <td>4</td>\n",
       "      <td>1355</td>\n",
       "      <td>5</td>\n",
       "      <td>28</td>\n",
       "      <td>6</td>\n",
       "      <td>52</td>\n",
       "      <td>154</td>\n",
       "      <td>462</td>\n",
       "      <td>33</td>\n",
       "      <td>89</td>\n",
       "      <td>78</td>\n",
       "      <td>285</td>\n",
       "      <td>16</td>\n",
       "      <td>145</td>\n",
       "      <td>95</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>23</td>\n",
       "      <td>22</td>\n",
       "      <td>12</td>\n",
       "      <td>272</td>\n",
       "      <td>40</td>\n",
       "      <td>57</td>\n",
       "      <td>31</td>\n",
       "      <td>11</td>\n",
       "      <td>4</td>\n",
       "      <td>22</td>\n",
       "      <td>47</td>\n",
       "      <td>6</td>\n",
       "      <td>2307</td>\n",
       "      <td>51</td>\n",
       "      <td>9</td>\n",
       "      <td>170</td>\n",
       "      <td>23</td>\n",
       "      <td>595</td>\n",
       "      <td>116</td>\n",
       "      <td>595</td>\n",
       "      <td>1352</td>\n",
       "      <td>13</td>\n",
       "      <td>191</td>\n",
       "      <td>79</td>\n",
       "      <td>638</td>\n",
       "      <td>89</td>\n",
       "      <td>2</td>\n",
       "      <td>14</td>\n",
       "      <td>9</td>\n",
       "      <td>8</td>\n",
       "      <td>106</td>\n",
       "      <td>607</td>\n",
       "      <td>624</td>\n",
       "      <td>35</td>\n",
       "      <td>534</td>\n",
       "      <td>6</td>\n",
       "      <td>227</td>\n",
       "      <td>7</td>\n",
       "      <td>129</td>\n",
       "      <td>113</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>12</td>\n",
       "      <td>47</td>\n",
       "      <td>6</td>\n",
       "      <td>2683</td>\n",
       "      <td>936</td>\n",
       "      <td>5</td>\n",
       "      <td>6307</td>\n",
       "      <td>2</td>\n",
       "      <td>19</td>\n",
       "      <td>49</td>\n",
       "      <td>7</td>\n",
       "      <td>4</td>\n",
       "      <td>1885</td>\n",
       "      <td>2</td>\n",
       "      <td>1118</td>\n",
       "      <td>25</td>\n",
       "      <td>80</td>\n",
       "      <td>126</td>\n",
       "      <td>842</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>4726</td>\n",
       "      <td>27</td>\n",
       "      <td>4494</td>\n",
       "      <td>11</td>\n",
       "      <td>1550</td>\n",
       "      <td>3633</td>\n",
       "      <td>159</td>\n",
       "      <td>27</td>\n",
       "      <td>341</td>\n",
       "      <td>29</td>\n",
       "      <td>2733</td>\n",
       "      <td>19</td>\n",
       "      <td>4185</td>\n",
       "      <td>173</td>\n",
       "      <td>7</td>\n",
       "      <td>90</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>14</td>\n",
       "      <td>31</td>\n",
       "      <td>9</td>\n",
       "      <td>242</td>\n",
       "      <td>955</td>\n",
       "      <td>48</td>\n",
       "      <td>25</td>\n",
       "      <td>279</td>\n",
       "      <td>2</td>\n",
       "      <td>23</td>\n",
       "      <td>12</td>\n",
       "      <td>1685</td>\n",
       "      <td>195</td>\n",
       "      <td>25</td>\n",
       "      <td>238</td>\n",
       "      <td>60</td>\n",
       "      <td>796</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>671</td>\n",
       "      <td>7</td>\n",
       "      <td>2804</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>559</td>\n",
       "      <td>154</td>\n",
       "      <td>888</td>\n",
       "      <td>7</td>\n",
       "      <td>726</td>\n",
       "      <td>50</td>\n",
       "      <td>26</td>\n",
       "      <td>49</td>\n",
       "      <td>7008</td>\n",
       "      <td>15</td>\n",
       "      <td>566</td>\n",
       "      <td>30</td>\n",
       "      <td>579</td>\n",
       "      <td>21</td>\n",
       "      <td>64</td>\n",
       "      <td>2574</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>74</td>\n",
       "      <td>233</td>\n",
       "      <td>334</td>\n",
       "      <td>207</td>\n",
       "      <td>126</td>\n",
       "      <td>224</td>\n",
       "      <td>12</td>\n",
       "      <td>562</td>\n",
       "      <td>298</td>\n",
       "      <td>2167</td>\n",
       "      <td>1272</td>\n",
       "      <td>7</td>\n",
       "      <td>2601</td>\n",
       "      <td>5</td>\n",
       "      <td>516</td>\n",
       "      <td>988</td>\n",
       "      <td>43</td>\n",
       "      <td>8</td>\n",
       "      <td>79</td>\n",
       "      <td>120</td>\n",
       "      <td>15</td>\n",
       "      <td>595</td>\n",
       "      <td>13</td>\n",
       "      <td>784</td>\n",
       "      <td>25</td>\n",
       "      <td>3171</td>\n",
       "      <td>18</td>\n",
       "      <td>165</td>\n",
       "      <td>170</td>\n",
       "      <td>143</td>\n",
       "      <td>19</td>\n",
       "      <td>14</td>\n",
       "      <td>5</td>\n",
       "      <td>7224</td>\n",
       "      <td>6</td>\n",
       "      <td>226</td>\n",
       "      <td>251</td>\n",
       "      <td>7</td>\n",
       "      <td>61</td>\n",
       "      <td>113</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 300 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   0    1    2     3    4    5     6    ...   293  294  295   296  297  298   299\n",
       "0    0    0    0     0    0    0     0  ...    32   15   16  5345   19  178    32\n",
       "1    0    0    0     0    0    0     0  ...    33   89   78   285   16  145    95\n",
       "2    0    0    0     0    0    0     0  ...    35  534    6   227    7  129   113\n",
       "3   12   47    6  2683  936    5  6307  ...    15  566   30   579   21   64  2574\n",
       "4    0    0    0     0    0    0     0  ...  7224    6  226   251    7   61   113\n",
       "\n",
       "[5 rows x 300 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "pd.DataFrame(x_train).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 253
    },
    "id": "5HdNDphTnkYw",
    "outputId": "fb29f646-031e-4115-8559-4152b5e080af"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "      <th>15</th>\n",
       "      <th>16</th>\n",
       "      <th>17</th>\n",
       "      <th>18</th>\n",
       "      <th>19</th>\n",
       "      <th>20</th>\n",
       "      <th>21</th>\n",
       "      <th>22</th>\n",
       "      <th>23</th>\n",
       "      <th>24</th>\n",
       "      <th>25</th>\n",
       "      <th>26</th>\n",
       "      <th>27</th>\n",
       "      <th>28</th>\n",
       "      <th>29</th>\n",
       "      <th>30</th>\n",
       "      <th>31</th>\n",
       "      <th>32</th>\n",
       "      <th>33</th>\n",
       "      <th>34</th>\n",
       "      <th>35</th>\n",
       "      <th>36</th>\n",
       "      <th>37</th>\n",
       "      <th>38</th>\n",
       "      <th>39</th>\n",
       "      <th>...</th>\n",
       "      <th>260</th>\n",
       "      <th>261</th>\n",
       "      <th>262</th>\n",
       "      <th>263</th>\n",
       "      <th>264</th>\n",
       "      <th>265</th>\n",
       "      <th>266</th>\n",
       "      <th>267</th>\n",
       "      <th>268</th>\n",
       "      <th>269</th>\n",
       "      <th>270</th>\n",
       "      <th>271</th>\n",
       "      <th>272</th>\n",
       "      <th>273</th>\n",
       "      <th>274</th>\n",
       "      <th>275</th>\n",
       "      <th>276</th>\n",
       "      <th>277</th>\n",
       "      <th>278</th>\n",
       "      <th>279</th>\n",
       "      <th>280</th>\n",
       "      <th>281</th>\n",
       "      <th>282</th>\n",
       "      <th>283</th>\n",
       "      <th>284</th>\n",
       "      <th>285</th>\n",
       "      <th>286</th>\n",
       "      <th>287</th>\n",
       "      <th>288</th>\n",
       "      <th>289</th>\n",
       "      <th>290</th>\n",
       "      <th>291</th>\n",
       "      <th>292</th>\n",
       "      <th>293</th>\n",
       "      <th>294</th>\n",
       "      <th>295</th>\n",
       "      <th>296</th>\n",
       "      <th>297</th>\n",
       "      <th>298</th>\n",
       "      <th>299</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>13</td>\n",
       "      <td>92</td>\n",
       "      <td>124</td>\n",
       "      <td>89</td>\n",
       "      <td>488</td>\n",
       "      <td>7944</td>\n",
       "      <td>100</td>\n",
       "      <td>28</td>\n",
       "      <td>1668</td>\n",
       "      <td>14</td>\n",
       "      <td>31</td>\n",
       "      <td>23</td>\n",
       "      <td>27</td>\n",
       "      <td>7479</td>\n",
       "      <td>29</td>\n",
       "      <td>220</td>\n",
       "      <td>468</td>\n",
       "      <td>8</td>\n",
       "      <td>124</td>\n",
       "      <td>14</td>\n",
       "      <td>286</td>\n",
       "      <td>170</td>\n",
       "      <td>8</td>\n",
       "      <td>157</td>\n",
       "      <td>46</td>\n",
       "      <td>5</td>\n",
       "      <td>27</td>\n",
       "      <td>239</td>\n",
       "      <td>16</td>\n",
       "      <td>179</td>\n",
       "      <td>2</td>\n",
       "      <td>38</td>\n",
       "      <td>32</td>\n",
       "      <td>25</td>\n",
       "      <td>7944</td>\n",
       "      <td>451</td>\n",
       "      <td>202</td>\n",
       "      <td>14</td>\n",
       "      <td>6</td>\n",
       "      <td>717</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>4</td>\n",
       "      <td>529</td>\n",
       "      <td>48</td>\n",
       "      <td>25</td>\n",
       "      <td>181</td>\n",
       "      <td>8</td>\n",
       "      <td>67</td>\n",
       "      <td>35</td>\n",
       "      <td>1732</td>\n",
       "      <td>22</td>\n",
       "      <td>49</td>\n",
       "      <td>238</td>\n",
       "      <td>60</td>\n",
       "      <td>135</td>\n",
       "      <td>1162</td>\n",
       "      <td>14</td>\n",
       "      <td>9</td>\n",
       "      <td>290</td>\n",
       "      <td>4</td>\n",
       "      <td>58</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>472</td>\n",
       "      <td>45</td>\n",
       "      <td>55</td>\n",
       "      <td>878</td>\n",
       "      <td>8</td>\n",
       "      <td>169</td>\n",
       "      <td>11</td>\n",
       "      <td>374</td>\n",
       "      <td>5687</td>\n",
       "      <td>25</td>\n",
       "      <td>203</td>\n",
       "      <td>28</td>\n",
       "      <td>8</td>\n",
       "      <td>818</td>\n",
       "      <td>12</td>\n",
       "      <td>125</td>\n",
       "      <td>4</td>\n",
       "      <td>3077</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1239</td>\n",
       "      <td>5189</td>\n",
       "      <td>137</td>\n",
       "      <td>2</td>\n",
       "      <td>18</td>\n",
       "      <td>27</td>\n",
       "      <td>173</td>\n",
       "      <td>9</td>\n",
       "      <td>2399</td>\n",
       "      <td>17</td>\n",
       "      <td>6</td>\n",
       "      <td>2</td>\n",
       "      <td>428</td>\n",
       "      <td>2</td>\n",
       "      <td>232</td>\n",
       "      <td>11</td>\n",
       "      <td>4</td>\n",
       "      <td>8014</td>\n",
       "      <td>37</td>\n",
       "      <td>272</td>\n",
       "      <td>40</td>\n",
       "      <td>2708</td>\n",
       "      <td>247</td>\n",
       "      <td>30</td>\n",
       "      <td>656</td>\n",
       "      <td>6</td>\n",
       "      <td>2</td>\n",
       "      <td>54</td>\n",
       "      <td>2</td>\n",
       "      <td>3292</td>\n",
       "      <td>98</td>\n",
       "      <td>6</td>\n",
       "      <td>2840</td>\n",
       "      <td>40</td>\n",
       "      <td>558</td>\n",
       "      <td>37</td>\n",
       "      <td>6093</td>\n",
       "      <td>98</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>2321</td>\n",
       "      <td>42</td>\n",
       "      <td>1898</td>\n",
       "      <td>11</td>\n",
       "      <td>4</td>\n",
       "      <td>3814</td>\n",
       "      <td>42</td>\n",
       "      <td>101</td>\n",
       "      <td>704</td>\n",
       "      <td>7</td>\n",
       "      <td>101</td>\n",
       "      <td>999</td>\n",
       "      <td>15</td>\n",
       "      <td>1625</td>\n",
       "      <td>94</td>\n",
       "      <td>2926</td>\n",
       "      <td>180</td>\n",
       "      <td>5</td>\n",
       "      <td>9</td>\n",
       "      <td>9101</td>\n",
       "      <td>34</td>\n",
       "      <td>2</td>\n",
       "      <td>45</td>\n",
       "      <td>6</td>\n",
       "      <td>1429</td>\n",
       "      <td>22</td>\n",
       "      <td>60</td>\n",
       "      <td>6</td>\n",
       "      <td>1220</td>\n",
       "      <td>31</td>\n",
       "      <td>11</td>\n",
       "      <td>94</td>\n",
       "      <td>6408</td>\n",
       "      <td>96</td>\n",
       "      <td>21</td>\n",
       "      <td>94</td>\n",
       "      <td>749</td>\n",
       "      <td>9</td>\n",
       "      <td>57</td>\n",
       "      <td>975</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>7</td>\n",
       "      <td>149</td>\n",
       "      <td>102</td>\n",
       "      <td>15</td>\n",
       "      <td>26</td>\n",
       "      <td>814</td>\n",
       "      <td>38</td>\n",
       "      <td>465</td>\n",
       "      <td>1627</td>\n",
       "      <td>31</td>\n",
       "      <td>70</td>\n",
       "      <td>983</td>\n",
       "      <td>67</td>\n",
       "      <td>51</td>\n",
       "      <td>9</td>\n",
       "      <td>112</td>\n",
       "      <td>814</td>\n",
       "      <td>17</td>\n",
       "      <td>35</td>\n",
       "      <td>311</td>\n",
       "      <td>75</td>\n",
       "      <td>26</td>\n",
       "      <td>2</td>\n",
       "      <td>574</td>\n",
       "      <td>19</td>\n",
       "      <td>4</td>\n",
       "      <td>1729</td>\n",
       "      <td>23</td>\n",
       "      <td>4</td>\n",
       "      <td>268</td>\n",
       "      <td>38</td>\n",
       "      <td>95</td>\n",
       "      <td>138</td>\n",
       "      <td>4</td>\n",
       "      <td>609</td>\n",
       "      <td>191</td>\n",
       "      <td>75</td>\n",
       "      <td>28</td>\n",
       "      <td>314</td>\n",
       "      <td>1772</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>401</td>\n",
       "      <td>14</td>\n",
       "      <td>1361</td>\n",
       "      <td>879</td>\n",
       "      <td>13</td>\n",
       "      <td>28</td>\n",
       "      <td>8</td>\n",
       "      <td>401</td>\n",
       "      <td>61</td>\n",
       "      <td>1642</td>\n",
       "      <td>2925</td>\n",
       "      <td>44</td>\n",
       "      <td>1373</td>\n",
       "      <td>21</td>\n",
       "      <td>591</td>\n",
       "      <td>353</td>\n",
       "      <td>14</td>\n",
       "      <td>500</td>\n",
       "      <td>4092</td>\n",
       "      <td>30</td>\n",
       "      <td>290</td>\n",
       "      <td>12</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>65</td>\n",
       "      <td>790</td>\n",
       "      <td>790</td>\n",
       "      <td>206</td>\n",
       "      <td>158</td>\n",
       "      <td>300</td>\n",
       "      <td>45</td>\n",
       "      <td>15</td>\n",
       "      <td>52</td>\n",
       "      <td>2</td>\n",
       "      <td>158</td>\n",
       "      <td>692</td>\n",
       "      <td>2</td>\n",
       "      <td>158</td>\n",
       "      <td>856</td>\n",
       "      <td>158</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 300 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    0     1    2    3    4    5    6    ...  293   294  295  296  297  298   299\n",
       "0     0     0    0    0    0    0    0  ...   25  7944  451  202   14    6   717\n",
       "1     0     0    0    0    0    0    0  ...   28     8  818   12  125    4  3077\n",
       "2  1239  5189  137    2   18   27  173  ...   96    21   94  749    9   57   975\n",
       "3     0     0    0    0    0    0    0  ...    4   609  191   75   28  314  1772\n",
       "4     0     0    0    0    0    0    0  ...    2   158  692    2  158  856   158\n",
       "\n",
       "[5 rows x 300 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(x_test).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "id": "HHyY5NpRnkYx",
    "outputId": "17d96f93-7e2d-4a72-8c2b-612b45a8a87e"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   0\n",
       "0  1\n",
       "1  0\n",
       "2  0\n",
       "3  1\n",
       "4  0"
      ]
     },
     "execution_count": 10,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(y_train).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "id": "xxslwJO8nkYy",
    "outputId": "c7750c14-45f1-415b-efb0-e2c51245914a"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   0\n",
       "0  0\n",
       "1  1\n",
       "2  1\n",
       "3  0\n",
       "4  1"
      ]
     },
     "execution_count": 11,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(y_test).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "LK9r-qd41BnD",
    "outputId": "e129f2c9-384b-4bb8-a77f-518bd41be7bb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Types of catagories: [0 1]\n",
      "Number of unique words after padding: 9999\n"
     ]
    }
   ],
   "source": [
    "# Concatinating both train and test data to get into the the dataset\n",
    "# It is to be noted that, both train and test set shape is same.\n",
    "\n",
    "features = np.concatenate((x_train, x_test), axis=0)\n",
    "target = np.concatenate((y_train, y_test), axis=0)\n",
    "\n",
    "print(\"Types of catagories:\", np.unique(target))\n",
    "print(\"Number of unique words after padding:\", len(np.unique(features)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fzB-_rh8nNWf"
   },
   "source": [
    "- After pading and considering the 10000 word limit, the number of of unique words reduced to 9999 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_-JDq7JzED-A",
    "outputId": "27a73486-719f-4fa0-d535-a5d82338fe66"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label: 0\n",
      "[   0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    1 1255 1223 5547\n",
      " 1265 2390 1747    8    4  268  103   27  532 3670 2016   14   58   45\n",
      "    6   22  198  256   18    2  137   23    6 8116 1189 5547  659    4\n",
      "  648    7    6 1976  936   33    4 1324    7    4 2019   41 2194   17\n",
      "   25  238  535    2   11    2 5547 1241   23    4  420  103  112 2633\n",
      "   34  132 2527    2    2 5547  659  309 1241   33 2960   17   29   47\n",
      "    8 3928   49 3493 2110   37  471    4 8915   23   90    5   29    9\n",
      "  309  258    8   30    4  293 1781   29  215  150  140   23    4  521\n",
      "    5  440    8 3330    4  420  584    4    2 2390  497  254    8 2225\n",
      "  178    4  835  640    5   97  178  264   29    9    6   52 1255  449\n",
      "    5   24    8  760  112 3375 1564    8    4    2    2 9753   21   29\n",
      "  996 3605   11   14 4895    2 6886 1187    4 2035 1322 7226   15    2\n",
      "  175  668    9  179 6424  175  255   11    4   22    9    6    2  419\n",
      "   37 1232    2  120    9 1011    7    4  370  167 1768    7  265    2\n",
      "   11    2   11   23    4 7886    7   68    2  257   58   17   36    2\n",
      "   68    2    2  225   60    6 3931 2723  992  109   15    2 2607   14\n",
      "   16    6   22   93   34  147  349   18  147  349    8    2   68  205\n",
      "    2 3124   45    6 7848    2]\n"
     ]
    }
   ],
   "source": [
    "# Let's check for any random training example\n",
    "\n",
    "print(\"Label:\", target[35])\n",
    "\n",
    "Label: 1\n",
    "print(features[35])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Iz7f4ctynl1x"
   },
   "source": [
    "- the zeros shown in the above output section is due to pading operation to meet the maximum length of words to be 300."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wBoZr3xWHTfE",
    "outputId": "d3c71bb8-1025-4794-e153-cc8d45b83e35"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label: 1\n",
      "[  15 1194    4   22    9  441   18    4   91  173   19  660 7141    4\n",
      " 3049  613    4 5789 1941   17   36  173    5    6  176   15  571  103\n",
      "   15    4  107 1529  531  238   30  717    2    2  103   59 1085    2\n",
      "    5 4759    2   21   50   71  407  111   85  211   54   13 1498  141\n",
      "   17  338    2  269    8    2   19   85 3321    4 5789  131   28   68\n",
      " 1382    5 2849 3871   63  540   82 1525 1671    4   20   18    4   91\n",
      "  173   17  127    4  114    6  606   21 1774   31   18   32 2088   50\n",
      "   26   49 6118  388  141   17    4 5582 5412  720    5    2    2  239\n",
      "   17 4759    9    2   21 1082    7  134  712   26   99 2681    5   26\n",
      "  230   39  195    8 2457    4  436  585   10   10   13   62  135    4\n",
      " 5582   20    4   22   15  645    4 3135   11 5515    9    4  118    7\n",
      "    4  204 2355    5   15  186    8   30    4   91 1063  652   14  840\n",
      "   22    9  242    4 4343    7    4  289   21   32    7   98   26   52\n",
      " 1025 5789   39  834    4  840    7    4 2253  108   11    4 3135   93\n",
      "  103    2  619 2581   33  222    4 5789  193 3754    9  131    4 5789\n",
      "   13  528  140   83 1373   44   51   13  104    7    4    2 4206   22\n",
      "  626 1784  153  103   68   86   31  237  207  460 1852   11   61  733\n",
      "    7   12  138   13  258   12   38 1332    5   60  151   12  127   28\n",
      "   49 1271  146  695   24  584  190  175 2253   20 1184    4 3204 5789\n",
      "   15   16   93  315    2  113    9   52  722   18    4  226  223   60\n",
      "   48    4  333    5  840 3668  257 1177    6 3382 6255   11  489  103\n",
      "    4   31   15 2555    2   12]\n"
     ]
    }
   ],
   "source": [
    "# Let's check for any random training example\n",
    "\n",
    "print(\"Label:\", target[40])\n",
    "\n",
    "Label: 0\n",
    "print(features[40])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0i_RbFMGn35Z"
   },
   "source": [
    "- In this feature, we have 300 words preesent in it, so pading is not required for it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Hqknsc94IPVi",
    "outputId": "77e03d39-f90c-499b-e6ea-a65acd0f2112"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/imdb_word_index.json\n",
      "1646592/1641221 [==============================] - 0s 0us/step\n"
     ]
    }
   ],
   "source": [
    "# Getting all the unique words and thier frequencies in the whole dataset\n",
    "key_value = imdb.get_word_index()\n",
    "\n",
    "# Reversing the key, value pairs\n",
    "reverse = dict()\n",
    "for (key, value) in key_value.items():\n",
    "    reverse.update([(value, key)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "pHqDU9jLIqig",
    "outputId": "359407c9-73f1-4126-bfe2-5c1a4ddf773c"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{34701: 'fawn',\n",
       " 52006: 'tsukino',\n",
       " 52007: 'nunnery',\n",
       " 16816: 'sonja',\n",
       " 63951: 'vani',\n",
       " 1408: 'woods',\n",
       " 16115: 'spiders',\n",
       " 2345: 'hanging',\n",
       " 2289: 'woody',\n",
       " 52008: 'trawling',\n",
       " 52009: \"hold's\",\n",
       " 11307: 'comically',\n",
       " 40830: 'localized',\n",
       " 30568: 'disobeying',\n",
       " 52010: \"'royale\",\n",
       " 40831: \"harpo's\",\n",
       " 52011: 'canet',\n",
       " 19313: 'aileen',\n",
       " 52012: 'acurately',\n",
       " 52013: \"diplomat's\",\n",
       " 25242: 'rickman',\n",
       " 6746: 'arranged',\n",
       " 52014: 'rumbustious',\n",
       " 52015: 'familiarness',\n",
       " 52016: \"spider'\",\n",
       " 68804: 'hahahah',\n",
       " 52017: \"wood'\",\n",
       " 40833: 'transvestism',\n",
       " 34702: \"hangin'\",\n",
       " 2338: 'bringing',\n",
       " 40834: 'seamier',\n",
       " 34703: 'wooded',\n",
       " 52018: 'bravora',\n",
       " 16817: 'grueling',\n",
       " 1636: 'wooden',\n",
       " 16818: 'wednesday',\n",
       " 52019: \"'prix\",\n",
       " 34704: 'altagracia',\n",
       " 52020: 'circuitry',\n",
       " 11585: 'crotch',\n",
       " 57766: 'busybody',\n",
       " 52021: \"tart'n'tangy\",\n",
       " 14129: 'burgade',\n",
       " 52023: 'thrace',\n",
       " 11038: \"tom's\",\n",
       " 52025: 'snuggles',\n",
       " 29114: 'francesco',\n",
       " 52027: 'complainers',\n",
       " 52125: 'templarios',\n",
       " 40835: '272',\n",
       " 52028: '273',\n",
       " 52130: 'zaniacs',\n",
       " 34706: '275',\n",
       " 27631: 'consenting',\n",
       " 40836: 'snuggled',\n",
       " 15492: 'inanimate',\n",
       " 52030: 'uality',\n",
       " 11926: 'bronte',\n",
       " 4010: 'errors',\n",
       " 3230: 'dialogs',\n",
       " 52031: \"yomada's\",\n",
       " 34707: \"madman's\",\n",
       " 30585: 'dialoge',\n",
       " 52033: 'usenet',\n",
       " 40837: 'videodrome',\n",
       " 26338: \"kid'\",\n",
       " 52034: 'pawed',\n",
       " 30569: \"'girlfriend'\",\n",
       " 52035: \"'pleasure\",\n",
       " 52036: \"'reloaded'\",\n",
       " 40839: \"kazakos'\",\n",
       " 52037: 'rocque',\n",
       " 52038: 'mailings',\n",
       " 11927: 'brainwashed',\n",
       " 16819: 'mcanally',\n",
       " 52039: \"tom''\",\n",
       " 25243: 'kurupt',\n",
       " 21905: 'affiliated',\n",
       " 52040: 'babaganoosh',\n",
       " 40840: \"noe's\",\n",
       " 40841: 'quart',\n",
       " 359: 'kids',\n",
       " 5034: 'uplifting',\n",
       " 7093: 'controversy',\n",
       " 21906: 'kida',\n",
       " 23379: 'kidd',\n",
       " 52041: \"error'\",\n",
       " 52042: 'neurologist',\n",
       " 18510: 'spotty',\n",
       " 30570: 'cobblers',\n",
       " 9878: 'projection',\n",
       " 40842: 'fastforwarding',\n",
       " 52043: 'sters',\n",
       " 52044: \"eggar's\",\n",
       " 52045: 'etherything',\n",
       " 40843: 'gateshead',\n",
       " 34708: 'airball',\n",
       " 25244: 'unsinkable',\n",
       " 7180: 'stern',\n",
       " 52046: \"cervi's\",\n",
       " 40844: 'dnd',\n",
       " 11586: 'dna',\n",
       " 20598: 'insecurity',\n",
       " 52047: \"'reboot'\",\n",
       " 11037: 'trelkovsky',\n",
       " 52048: 'jaekel',\n",
       " 52049: 'sidebars',\n",
       " 52050: \"sforza's\",\n",
       " 17633: 'distortions',\n",
       " 52051: 'mutinies',\n",
       " 30602: 'sermons',\n",
       " 40846: '7ft',\n",
       " 52052: 'boobage',\n",
       " 52053: \"o'bannon's\",\n",
       " 23380: 'populations',\n",
       " 52054: 'chulak',\n",
       " 27633: 'mesmerize',\n",
       " 52055: 'quinnell',\n",
       " 10307: 'yahoo',\n",
       " 52057: 'meteorologist',\n",
       " 42577: 'beswick',\n",
       " 15493: 'boorman',\n",
       " 40847: 'voicework',\n",
       " 52058: \"ster'\",\n",
       " 22922: 'blustering',\n",
       " 52059: 'hj',\n",
       " 27634: 'intake',\n",
       " 5621: 'morally',\n",
       " 40849: 'jumbling',\n",
       " 52060: 'bowersock',\n",
       " 52061: \"'porky's'\",\n",
       " 16821: 'gershon',\n",
       " 40850: 'ludicrosity',\n",
       " 52062: 'coprophilia',\n",
       " 40851: 'expressively',\n",
       " 19500: \"india's\",\n",
       " 34710: \"post's\",\n",
       " 52063: 'wana',\n",
       " 5283: 'wang',\n",
       " 30571: 'wand',\n",
       " 25245: 'wane',\n",
       " 52321: 'edgeways',\n",
       " 34711: 'titanium',\n",
       " 40852: 'pinta',\n",
       " 178: 'want',\n",
       " 30572: 'pinto',\n",
       " 52065: 'whoopdedoodles',\n",
       " 21908: 'tchaikovsky',\n",
       " 2103: 'travel',\n",
       " 52066: \"'victory'\",\n",
       " 11928: 'copious',\n",
       " 22433: 'gouge',\n",
       " 52067: \"chapters'\",\n",
       " 6702: 'barbra',\n",
       " 30573: 'uselessness',\n",
       " 52068: \"wan'\",\n",
       " 27635: 'assimilated',\n",
       " 16116: 'petiot',\n",
       " 52069: 'most\\x85and',\n",
       " 3930: 'dinosaurs',\n",
       " 352: 'wrong',\n",
       " 52070: 'seda',\n",
       " 52071: 'stollen',\n",
       " 34712: 'sentencing',\n",
       " 40853: 'ouroboros',\n",
       " 40854: 'assimilates',\n",
       " 40855: 'colorfully',\n",
       " 27636: 'glenne',\n",
       " 52072: 'dongen',\n",
       " 4760: 'subplots',\n",
       " 52073: 'kiloton',\n",
       " 23381: 'chandon',\n",
       " 34713: \"effect'\",\n",
       " 27637: 'snugly',\n",
       " 40856: 'kuei',\n",
       " 9092: 'welcomed',\n",
       " 30071: 'dishonor',\n",
       " 52075: 'concurrence',\n",
       " 23382: 'stoicism',\n",
       " 14896: \"guys'\",\n",
       " 52077: \"beroemd'\",\n",
       " 6703: 'butcher',\n",
       " 40857: \"melfi's\",\n",
       " 30623: 'aargh',\n",
       " 20599: 'playhouse',\n",
       " 11308: 'wickedly',\n",
       " 1180: 'fit',\n",
       " 52078: 'labratory',\n",
       " 40859: 'lifeline',\n",
       " 1927: 'screaming',\n",
       " 4287: 'fix',\n",
       " 52079: 'cineliterate',\n",
       " 52080: 'fic',\n",
       " 52081: 'fia',\n",
       " 34714: 'fig',\n",
       " 52082: 'fmvs',\n",
       " 52083: 'fie',\n",
       " 52084: 'reentered',\n",
       " 30574: 'fin',\n",
       " 52085: 'doctresses',\n",
       " 52086: 'fil',\n",
       " 12606: 'zucker',\n",
       " 31931: 'ached',\n",
       " 52088: 'counsil',\n",
       " 52089: 'paterfamilias',\n",
       " 13885: 'songwriter',\n",
       " 34715: 'shivam',\n",
       " 9654: 'hurting',\n",
       " 299: 'effects',\n",
       " 52090: 'slauther',\n",
       " 52091: \"'flame'\",\n",
       " 52092: 'sommerset',\n",
       " 52093: 'interwhined',\n",
       " 27638: 'whacking',\n",
       " 52094: 'bartok',\n",
       " 8775: 'barton',\n",
       " 21909: 'frewer',\n",
       " 52095: \"fi'\",\n",
       " 6192: 'ingrid',\n",
       " 30575: 'stribor',\n",
       " 52096: 'approporiately',\n",
       " 52097: 'wobblyhand',\n",
       " 52098: 'tantalisingly',\n",
       " 52099: 'ankylosaurus',\n",
       " 17634: 'parasites',\n",
       " 52100: 'childen',\n",
       " 52101: \"jenkins'\",\n",
       " 52102: 'metafiction',\n",
       " 17635: 'golem',\n",
       " 40860: 'indiscretion',\n",
       " 23383: \"reeves'\",\n",
       " 57781: \"inamorata's\",\n",
       " 52104: 'brittannica',\n",
       " 7916: 'adapt',\n",
       " 30576: \"russo's\",\n",
       " 48246: 'guitarists',\n",
       " 10553: 'abbott',\n",
       " 40861: 'abbots',\n",
       " 17649: 'lanisha',\n",
       " 40863: 'magickal',\n",
       " 52105: 'mattter',\n",
       " 52106: \"'willy\",\n",
       " 34716: 'pumpkins',\n",
       " 52107: 'stuntpeople',\n",
       " 30577: 'estimate',\n",
       " 40864: 'ugghhh',\n",
       " 11309: 'gameplay',\n",
       " 52108: \"wern't\",\n",
       " 40865: \"n'sync\",\n",
       " 16117: 'sickeningly',\n",
       " 40866: 'chiara',\n",
       " 4011: 'disturbed',\n",
       " 40867: 'portmanteau',\n",
       " 52109: 'ineffectively',\n",
       " 82143: \"duchonvey's\",\n",
       " 37519: \"nasty'\",\n",
       " 1285: 'purpose',\n",
       " 52112: 'lazers',\n",
       " 28105: 'lightened',\n",
       " 52113: 'kaliganj',\n",
       " 52114: 'popularism',\n",
       " 18511: \"damme's\",\n",
       " 30578: 'stylistics',\n",
       " 52115: 'mindgaming',\n",
       " 46449: 'spoilerish',\n",
       " 52117: \"'corny'\",\n",
       " 34718: 'boerner',\n",
       " 6792: 'olds',\n",
       " 52118: 'bakelite',\n",
       " 27639: 'renovated',\n",
       " 27640: 'forrester',\n",
       " 52119: \"lumiere's\",\n",
       " 52024: 'gaskets',\n",
       " 884: 'needed',\n",
       " 34719: 'smight',\n",
       " 1297: 'master',\n",
       " 25905: \"edie's\",\n",
       " 40868: 'seeber',\n",
       " 52120: 'hiya',\n",
       " 52121: 'fuzziness',\n",
       " 14897: 'genesis',\n",
       " 12607: 'rewards',\n",
       " 30579: 'enthrall',\n",
       " 40869: \"'about\",\n",
       " 52122: \"recollection's\",\n",
       " 11039: 'mutilated',\n",
       " 52123: 'fatherlands',\n",
       " 52124: \"fischer's\",\n",
       " 5399: 'positively',\n",
       " 34705: '270',\n",
       " 34720: 'ahmed',\n",
       " 9836: 'zatoichi',\n",
       " 13886: 'bannister',\n",
       " 52127: 'anniversaries',\n",
       " 30580: \"helm's\",\n",
       " 52128: \"'work'\",\n",
       " 34721: 'exclaimed',\n",
       " 52129: \"'unfunny'\",\n",
       " 52029: '274',\n",
       " 544: 'feeling',\n",
       " 52131: \"wanda's\",\n",
       " 33266: 'dolan',\n",
       " 52133: '278',\n",
       " 52134: 'peacoat',\n",
       " 40870: 'brawny',\n",
       " 40871: 'mishra',\n",
       " 40872: 'worlders',\n",
       " 52135: 'protags',\n",
       " 52136: 'skullcap',\n",
       " 57596: 'dastagir',\n",
       " 5622: 'affairs',\n",
       " 7799: 'wholesome',\n",
       " 52137: 'hymen',\n",
       " 25246: 'paramedics',\n",
       " 52138: 'unpersons',\n",
       " 52139: 'heavyarms',\n",
       " 52140: 'affaire',\n",
       " 52141: 'coulisses',\n",
       " 40873: 'hymer',\n",
       " 52142: 'kremlin',\n",
       " 30581: 'shipments',\n",
       " 52143: 'pixilated',\n",
       " 30582: \"'00s\",\n",
       " 18512: 'diminishing',\n",
       " 1357: 'cinematic',\n",
       " 14898: 'resonates',\n",
       " 40874: 'simplify',\n",
       " 40875: \"nature'\",\n",
       " 40876: 'temptresses',\n",
       " 16822: 'reverence',\n",
       " 19502: 'resonated',\n",
       " 34722: 'dailey',\n",
       " 52144: '2\\x85',\n",
       " 27641: 'treize',\n",
       " 52145: 'majo',\n",
       " 21910: 'kiya',\n",
       " 52146: 'woolnough',\n",
       " 39797: 'thanatos',\n",
       " 35731: 'sandoval',\n",
       " 40879: 'dorama',\n",
       " 52147: \"o'shaughnessy\",\n",
       " 4988: 'tech',\n",
       " 32018: 'fugitives',\n",
       " 30583: 'teck',\n",
       " 76125: \"'e'\",\n",
       " 40881: 'doesn’t',\n",
       " 52149: 'purged',\n",
       " 657: 'saying',\n",
       " 41095: \"martians'\",\n",
       " 23418: 'norliss',\n",
       " 27642: 'dickey',\n",
       " 52152: 'dicker',\n",
       " 52153: \"'sependipity\",\n",
       " 8422: 'padded',\n",
       " 57792: 'ordell',\n",
       " 40882: \"sturges'\",\n",
       " 52154: 'independentcritics',\n",
       " 5745: 'tempted',\n",
       " 34724: \"atkinson's\",\n",
       " 25247: 'hounded',\n",
       " 52155: 'apace',\n",
       " 15494: 'clicked',\n",
       " 30584: \"'humor'\",\n",
       " 17177: \"martino's\",\n",
       " 52156: \"'supporting\",\n",
       " 52032: 'warmongering',\n",
       " 34725: \"zemeckis's\",\n",
       " 21911: 'lube',\n",
       " 52157: 'shocky',\n",
       " 7476: 'plate',\n",
       " 40883: 'plata',\n",
       " 40884: 'sturgess',\n",
       " 40885: \"nerds'\",\n",
       " 20600: 'plato',\n",
       " 34726: 'plath',\n",
       " 40886: 'platt',\n",
       " 52159: 'mcnab',\n",
       " 27643: 'clumsiness',\n",
       " 3899: 'altogether',\n",
       " 42584: 'massacring',\n",
       " 52160: 'bicenntinial',\n",
       " 40887: 'skaal',\n",
       " 14360: 'droning',\n",
       " 8776: 'lds',\n",
       " 21912: 'jaguar',\n",
       " 34727: \"cale's\",\n",
       " 1777: 'nicely',\n",
       " 4588: 'mummy',\n",
       " 18513: \"lot's\",\n",
       " 10086: 'patch',\n",
       " 50202: 'kerkhof',\n",
       " 52161: \"leader's\",\n",
       " 27644: \"'movie\",\n",
       " 52162: 'uncomfirmed',\n",
       " 40888: 'heirloom',\n",
       " 47360: 'wrangle',\n",
       " 52163: 'emotion\\x85',\n",
       " 52164: \"'stargate'\",\n",
       " 40889: 'pinoy',\n",
       " 40890: 'conchatta',\n",
       " 41128: 'broeke',\n",
       " 40891: 'advisedly',\n",
       " 17636: \"barker's\",\n",
       " 52166: 'descours',\n",
       " 772: 'lots',\n",
       " 9259: 'lotr',\n",
       " 9879: 'irs',\n",
       " 52167: 'lott',\n",
       " 40892: 'xvi',\n",
       " 34728: 'irk',\n",
       " 52168: 'irl',\n",
       " 6887: 'ira',\n",
       " 21913: 'belzer',\n",
       " 52169: 'irc',\n",
       " 27645: 'ire',\n",
       " 40893: 'requisites',\n",
       " 7693: 'discipline',\n",
       " 52961: 'lyoko',\n",
       " 11310: 'extend',\n",
       " 873: 'nature',\n",
       " 52170: \"'dickie'\",\n",
       " 40894: 'optimist',\n",
       " 30586: 'lapping',\n",
       " 3900: 'superficial',\n",
       " 52171: 'vestment',\n",
       " 2823: 'extent',\n",
       " 52172: 'tendons',\n",
       " 52173: \"heller's\",\n",
       " 52174: 'quagmires',\n",
       " 52175: 'miyako',\n",
       " 20601: 'moocow',\n",
       " 52176: \"coles'\",\n",
       " 40895: 'lookit',\n",
       " 52177: 'ravenously',\n",
       " 40896: 'levitating',\n",
       " 52178: 'perfunctorily',\n",
       " 30587: 'lookin',\n",
       " 40898: \"lot'\",\n",
       " 52179: 'lookie',\n",
       " 34870: 'fearlessly',\n",
       " 52181: 'libyan',\n",
       " 40899: 'fondles',\n",
       " 35714: 'gopher',\n",
       " 40901: 'wearying',\n",
       " 52182: \"nz's\",\n",
       " 27646: 'minuses',\n",
       " 52183: 'puposelessly',\n",
       " 52184: 'shandling',\n",
       " 31268: 'decapitates',\n",
       " 11929: 'humming',\n",
       " 40902: \"'nother\",\n",
       " 21914: 'smackdown',\n",
       " 30588: 'underdone',\n",
       " 40903: 'frf',\n",
       " 52185: 'triviality',\n",
       " 25248: 'fro',\n",
       " 8777: 'bothers',\n",
       " 52186: \"'kensington\",\n",
       " 73: 'much',\n",
       " 34730: 'muco',\n",
       " 22615: 'wiseguy',\n",
       " 27648: \"richie's\",\n",
       " 40904: 'tonino',\n",
       " 52187: 'unleavened',\n",
       " 11587: 'fry',\n",
       " 40905: \"'tv'\",\n",
       " 40906: 'toning',\n",
       " 14361: 'obese',\n",
       " 30589: 'sensationalized',\n",
       " 40907: 'spiv',\n",
       " 6259: 'spit',\n",
       " 7364: 'arkin',\n",
       " 21915: 'charleton',\n",
       " 16823: 'jeon',\n",
       " 21916: 'boardroom',\n",
       " 4989: 'doubts',\n",
       " 3084: 'spin',\n",
       " 53083: 'hepo',\n",
       " 27649: 'wildcat',\n",
       " 10584: 'venoms',\n",
       " 52191: 'misconstrues',\n",
       " 18514: 'mesmerising',\n",
       " 40908: 'misconstrued',\n",
       " 52192: 'rescinds',\n",
       " 52193: 'prostrate',\n",
       " 40909: 'majid',\n",
       " 16479: 'climbed',\n",
       " 34731: 'canoeing',\n",
       " 52195: 'majin',\n",
       " 57804: 'animie',\n",
       " 40910: 'sylke',\n",
       " 14899: 'conditioned',\n",
       " 40911: 'waddell',\n",
       " 52196: '3\\x85',\n",
       " 41188: 'hyperdrive',\n",
       " 34732: 'conditioner',\n",
       " 53153: 'bricklayer',\n",
       " 2576: 'hong',\n",
       " 52198: 'memoriam',\n",
       " 30592: 'inventively',\n",
       " 25249: \"levant's\",\n",
       " 20638: 'portobello',\n",
       " 52200: 'remand',\n",
       " 19504: 'mummified',\n",
       " 27650: 'honk',\n",
       " 19505: 'spews',\n",
       " 40912: 'visitations',\n",
       " 52201: 'mummifies',\n",
       " 25250: 'cavanaugh',\n",
       " 23385: 'zeon',\n",
       " 40913: \"jungle's\",\n",
       " 34733: 'viertel',\n",
       " 27651: 'frenchmen',\n",
       " 52202: 'torpedoes',\n",
       " 52203: 'schlessinger',\n",
       " 34734: 'torpedoed',\n",
       " 69876: 'blister',\n",
       " 52204: 'cinefest',\n",
       " 34735: 'furlough',\n",
       " 52205: 'mainsequence',\n",
       " 40914: 'mentors',\n",
       " 9094: 'academic',\n",
       " 20602: 'stillness',\n",
       " 40915: 'academia',\n",
       " 52206: 'lonelier',\n",
       " 52207: 'nibby',\n",
       " 52208: \"losers'\",\n",
       " 40916: 'cineastes',\n",
       " 4449: 'corporate',\n",
       " 40917: 'massaging',\n",
       " 30593: 'bellow',\n",
       " 19506: 'absurdities',\n",
       " 53241: 'expetations',\n",
       " 40918: 'nyfiken',\n",
       " 75638: 'mehras',\n",
       " 52209: 'lasse',\n",
       " 52210: 'visability',\n",
       " 33946: 'militarily',\n",
       " 52211: \"elder'\",\n",
       " 19023: 'gainsbourg',\n",
       " 20603: 'hah',\n",
       " 13420: 'hai',\n",
       " 34736: 'haj',\n",
       " 25251: 'hak',\n",
       " 4311: 'hal',\n",
       " 4892: 'ham',\n",
       " 53259: 'duffer',\n",
       " 52213: 'haa',\n",
       " 66: 'had',\n",
       " 11930: 'advancement',\n",
       " 16825: 'hag',\n",
       " 25252: \"hand'\",\n",
       " 13421: 'hay',\n",
       " 20604: 'mcnamara',\n",
       " 52214: \"mozart's\",\n",
       " 30731: 'duffel',\n",
       " 30594: 'haq',\n",
       " 13887: 'har',\n",
       " 44: 'has',\n",
       " 2401: 'hat',\n",
       " 40919: 'hav',\n",
       " 30595: 'haw',\n",
       " 52215: 'figtings',\n",
       " 15495: 'elders',\n",
       " 52216: 'underpanted',\n",
       " 52217: 'pninson',\n",
       " 27652: 'unequivocally',\n",
       " 23673: \"barbara's\",\n",
       " 52219: \"bello'\",\n",
       " 12997: 'indicative',\n",
       " 40920: 'yawnfest',\n",
       " 52220: 'hexploitation',\n",
       " 52221: \"loder's\",\n",
       " 27653: 'sleuthing',\n",
       " 32622: \"justin's\",\n",
       " 52222: \"'ball\",\n",
       " 52223: \"'summer\",\n",
       " 34935: \"'demons'\",\n",
       " 52225: \"mormon's\",\n",
       " 34737: \"laughton's\",\n",
       " 52226: 'debell',\n",
       " 39724: 'shipyard',\n",
       " 30597: 'unabashedly',\n",
       " 40401: 'disks',\n",
       " 2290: 'crowd',\n",
       " 10087: 'crowe',\n",
       " 56434: \"vancouver's\",\n",
       " 34738: 'mosques',\n",
       " 6627: 'crown',\n",
       " 52227: 'culpas',\n",
       " 27654: 'crows',\n",
       " 53344: 'surrell',\n",
       " 52229: 'flowless',\n",
       " 52230: 'sheirk',\n",
       " 40923: \"'three\",\n",
       " 52231: \"peterson'\",\n",
       " 52232: 'ooverall',\n",
       " 40924: 'perchance',\n",
       " 1321: 'bottom',\n",
       " 53363: 'chabert',\n",
       " 52233: 'sneha',\n",
       " 13888: 'inhuman',\n",
       " 52234: 'ichii',\n",
       " 52235: 'ursla',\n",
       " 30598: 'completly',\n",
       " 40925: 'moviedom',\n",
       " 52236: 'raddick',\n",
       " 51995: 'brundage',\n",
       " 40926: 'brigades',\n",
       " 1181: 'starring',\n",
       " 52237: \"'goal'\",\n",
       " 52238: 'caskets',\n",
       " 52239: 'willcock',\n",
       " 52240: \"threesome's\",\n",
       " 52241: \"mosque'\",\n",
       " 52242: \"cover's\",\n",
       " 17637: 'spaceships',\n",
       " 40927: 'anomalous',\n",
       " 27655: 'ptsd',\n",
       " 52243: 'shirdan',\n",
       " 21962: 'obscenity',\n",
       " 30599: 'lemmings',\n",
       " 30600: 'duccio',\n",
       " 52244: \"levene's\",\n",
       " 52245: \"'gorby'\",\n",
       " 25255: \"teenager's\",\n",
       " 5340: 'marshall',\n",
       " 9095: 'honeymoon',\n",
       " 3231: 'shoots',\n",
       " 12258: 'despised',\n",
       " 52246: 'okabasho',\n",
       " 8289: 'fabric',\n",
       " 18515: 'cannavale',\n",
       " 3537: 'raped',\n",
       " 52247: \"tutt's\",\n",
       " 17638: 'grasping',\n",
       " 18516: 'despises',\n",
       " 40928: \"thief's\",\n",
       " 8926: 'rapes',\n",
       " 52248: 'raper',\n",
       " 27656: \"eyre'\",\n",
       " 52249: 'walchek',\n",
       " 23386: \"elmo's\",\n",
       " 40929: 'perfumes',\n",
       " 21918: 'spurting',\n",
       " 52250: \"exposition'\\x85\",\n",
       " 52251: 'denoting',\n",
       " 34740: 'thesaurus',\n",
       " 40930: \"shoot'\",\n",
       " 49759: 'bonejack',\n",
       " 52253: 'simpsonian',\n",
       " 30601: 'hebetude',\n",
       " 34741: \"hallow's\",\n",
       " 52254: 'desperation\\x85',\n",
       " 34742: 'incinerator',\n",
       " 10308: 'congratulations',\n",
       " 52255: 'humbled',\n",
       " 5924: \"else's\",\n",
       " 40845: 'trelkovski',\n",
       " 52256: \"rape'\",\n",
       " 59386: \"'chapters'\",\n",
       " 52257: '1600s',\n",
       " 7253: 'martian',\n",
       " 25256: 'nicest',\n",
       " 52259: 'eyred',\n",
       " 9457: 'passenger',\n",
       " 6041: 'disgrace',\n",
       " 52260: 'moderne',\n",
       " 5120: 'barrymore',\n",
       " 52261: 'yankovich',\n",
       " 40931: 'moderns',\n",
       " 52262: 'studliest',\n",
       " 52263: 'bedsheet',\n",
       " 14900: 'decapitation',\n",
       " 52264: 'slurring',\n",
       " 52265: \"'nunsploitation'\",\n",
       " 34743: \"'character'\",\n",
       " 9880: 'cambodia',\n",
       " 52266: 'rebelious',\n",
       " 27657: 'pasadena',\n",
       " 40932: 'crowne',\n",
       " 52267: \"'bedchamber\",\n",
       " 52268: 'conjectural',\n",
       " 52269: 'appologize',\n",
       " 52270: 'halfassing',\n",
       " 57816: 'paycheque',\n",
       " 20606: 'palms',\n",
       " 52271: \"'islands\",\n",
       " 40933: 'hawked',\n",
       " 21919: 'palme',\n",
       " 40934: 'conservatively',\n",
       " 64007: 'larp',\n",
       " 5558: 'palma',\n",
       " 21920: 'smelling',\n",
       " 12998: 'aragorn',\n",
       " 52272: 'hawker',\n",
       " 52273: 'hawkes',\n",
       " 3975: 'explosions',\n",
       " 8059: 'loren',\n",
       " 52274: \"pyle's\",\n",
       " 6704: 'shootout',\n",
       " 18517: \"mike's\",\n",
       " 52275: \"driscoll's\",\n",
       " 40935: 'cogsworth',\n",
       " 52276: \"britian's\",\n",
       " 34744: 'childs',\n",
       " 52277: \"portrait's\",\n",
       " 3626: 'chain',\n",
       " 2497: 'whoever',\n",
       " 52278: 'puttered',\n",
       " 52279: 'childe',\n",
       " 52280: 'maywether',\n",
       " 3036: 'chair',\n",
       " 52281: \"rance's\",\n",
       " 34745: 'machu',\n",
       " 4517: 'ballet',\n",
       " 34746: 'grapples',\n",
       " 76152: 'summerize',\n",
       " 30603: 'freelance',\n",
       " 52283: \"andrea's\",\n",
       " 52284: '\\x91very',\n",
       " 45879: 'coolidge',\n",
       " 18518: 'mache',\n",
       " 52285: 'balled',\n",
       " 40937: 'grappled',\n",
       " 18519: 'macha',\n",
       " 21921: 'underlining',\n",
       " 5623: 'macho',\n",
       " 19507: 'oversight',\n",
       " 25257: 'machi',\n",
       " 11311: 'verbally',\n",
       " 21922: 'tenacious',\n",
       " 40938: 'windshields',\n",
       " 18557: 'paychecks',\n",
       " 3396: 'jerk',\n",
       " 11931: \"good'\",\n",
       " 34748: 'prancer',\n",
       " 21923: 'prances',\n",
       " 52286: 'olympus',\n",
       " 21924: 'lark',\n",
       " 10785: 'embark',\n",
       " 7365: 'gloomy',\n",
       " 52287: 'jehaan',\n",
       " 52288: 'turaqui',\n",
       " 20607: \"child'\",\n",
       " 2894: 'locked',\n",
       " 52289: 'pranced',\n",
       " 2588: 'exact',\n",
       " 52290: 'unattuned',\n",
       " 783: 'minute',\n",
       " 16118: 'skewed',\n",
       " 40940: 'hodgins',\n",
       " 34749: 'skewer',\n",
       " 52291: 'think\\x85',\n",
       " 38765: 'rosenstein',\n",
       " 52292: 'helmit',\n",
       " 34750: 'wrestlemanias',\n",
       " 16826: 'hindered',\n",
       " 30604: \"martha's\",\n",
       " 52293: 'cheree',\n",
       " 52294: \"pluckin'\",\n",
       " 40941: 'ogles',\n",
       " 11932: 'heavyweight',\n",
       " 82190: 'aada',\n",
       " 11312: 'chopping',\n",
       " 61534: 'strongboy',\n",
       " 41342: 'hegemonic',\n",
       " 40942: 'adorns',\n",
       " 41346: 'xxth',\n",
       " 34751: 'nobuhiro',\n",
       " 52298: 'capitães',\n",
       " 52299: 'kavogianni',\n",
       " 13422: 'antwerp',\n",
       " 6538: 'celebrated',\n",
       " 52300: 'roarke',\n",
       " 40943: 'baggins',\n",
       " 31270: 'cheeseburgers',\n",
       " 52301: 'matras',\n",
       " 52302: \"nineties'\",\n",
       " 52303: \"'craig'\",\n",
       " 12999: 'celebrates',\n",
       " 3383: 'unintentionally',\n",
       " 14362: 'drafted',\n",
       " 52304: 'climby',\n",
       " 52305: '303',\n",
       " 18520: 'oldies',\n",
       " 9096: 'climbs',\n",
       " 9655: 'honour',\n",
       " 34752: 'plucking',\n",
       " 30074: '305',\n",
       " 5514: 'address',\n",
       " 40944: 'menjou',\n",
       " 42592: \"'freak'\",\n",
       " 19508: 'dwindling',\n",
       " 9458: 'benson',\n",
       " 52307: 'white’s',\n",
       " 40945: 'shamelessness',\n",
       " 21925: 'impacted',\n",
       " 52308: 'upatz',\n",
       " 3840: 'cusack',\n",
       " 37567: \"flavia's\",\n",
       " 52309: 'effette',\n",
       " 34753: 'influx',\n",
       " 52310: 'boooooooo',\n",
       " 52311: 'dimitrova',\n",
       " 13423: 'houseman',\n",
       " 25259: 'bigas',\n",
       " 52312: 'boylen',\n",
       " 52313: 'phillipenes',\n",
       " 40946: 'fakery',\n",
       " 27658: \"grandpa's\",\n",
       " 27659: 'darnell',\n",
       " 19509: 'undergone',\n",
       " 52315: 'handbags',\n",
       " 21926: 'perished',\n",
       " 37778: 'pooped',\n",
       " 27660: 'vigour',\n",
       " 3627: 'opposed',\n",
       " 52316: 'etude',\n",
       " 11799: \"caine's\",\n",
       " 52317: 'doozers',\n",
       " 34754: 'photojournals',\n",
       " 52318: 'perishes',\n",
       " 34755: 'constrains',\n",
       " 40948: 'migenes',\n",
       " 30605: 'consoled',\n",
       " 16827: 'alastair',\n",
       " 52319: 'wvs',\n",
       " 52320: 'ooooooh',\n",
       " 34756: 'approving',\n",
       " 40949: 'consoles',\n",
       " 52064: 'disparagement',\n",
       " 52322: 'futureistic',\n",
       " 52323: 'rebounding',\n",
       " 52324: \"'date\",\n",
       " 52325: 'gregoire',\n",
       " 21927: 'rutherford',\n",
       " 34757: 'americanised',\n",
       " 82196: 'novikov',\n",
       " 1042: 'following',\n",
       " 34758: 'munroe',\n",
       " 52326: \"morita'\",\n",
       " 52327: 'christenssen',\n",
       " 23106: 'oatmeal',\n",
       " 25260: 'fossey',\n",
       " 40950: 'livered',\n",
       " 13000: 'listens',\n",
       " 76164: \"'marci\",\n",
       " 52330: \"otis's\",\n",
       " 23387: 'thanking',\n",
       " 16019: 'maude',\n",
       " 34759: 'extensions',\n",
       " 52332: 'ameteurish',\n",
       " 52333: \"commender's\",\n",
       " 27661: 'agricultural',\n",
       " 4518: 'convincingly',\n",
       " 17639: 'fueled',\n",
       " 54014: 'mahattan',\n",
       " 40952: \"paris's\",\n",
       " 52336: 'vulkan',\n",
       " 52337: 'stapes',\n",
       " 52338: 'odysessy',\n",
       " 12259: 'harmon',\n",
       " 4252: 'surfing',\n",
       " 23494: 'halloran',\n",
       " 49580: 'unbelieveably',\n",
       " 52339: \"'offed'\",\n",
       " 30607: 'quadrant',\n",
       " 19510: 'inhabiting',\n",
       " 34760: 'nebbish',\n",
       " 40953: 'forebears',\n",
       " 34761: 'skirmish',\n",
       " 52340: 'ocassionally',\n",
       " 52341: \"'resist\",\n",
       " 21928: 'impactful',\n",
       " 52342: 'spicier',\n",
       " 40954: 'touristy',\n",
       " 52343: \"'football'\",\n",
       " 40955: 'webpage',\n",
       " 52345: 'exurbia',\n",
       " 52346: 'jucier',\n",
       " 14901: 'professors',\n",
       " 34762: 'structuring',\n",
       " 30608: 'jig',\n",
       " 40956: 'overlord',\n",
       " 25261: 'disconnect',\n",
       " 82201: 'sniffle',\n",
       " 40957: 'slimeball',\n",
       " 40958: 'jia',\n",
       " 16828: 'milked',\n",
       " 40959: 'banjoes',\n",
       " 1237: 'jim',\n",
       " 52348: 'workforces',\n",
       " 52349: 'jip',\n",
       " 52350: 'rotweiller',\n",
       " 34763: 'mundaneness',\n",
       " 52351: \"'ninja'\",\n",
       " 11040: \"dead'\",\n",
       " 40960: \"cipriani's\",\n",
       " 20608: 'modestly',\n",
       " 52352: \"professor'\",\n",
       " 40961: 'shacked',\n",
       " 34764: 'bashful',\n",
       " 23388: 'sorter',\n",
       " 16120: 'overpowering',\n",
       " 18521: 'workmanlike',\n",
       " 27662: 'henpecked',\n",
       " 18522: 'sorted',\n",
       " 52354: \"jōb's\",\n",
       " 52355: \"'always\",\n",
       " 34765: \"'baptists\",\n",
       " 52356: 'dreamcatchers',\n",
       " 52357: \"'silence'\",\n",
       " 21929: 'hickory',\n",
       " 52358: 'fun\\x97yet',\n",
       " 52359: 'breakumentary',\n",
       " 15496: 'didn',\n",
       " 52360: 'didi',\n",
       " 52361: 'pealing',\n",
       " 40962: 'dispite',\n",
       " 25262: \"italy's\",\n",
       " 21930: 'instability',\n",
       " 6539: 'quarter',\n",
       " 12608: 'quartet',\n",
       " 52362: 'padmé',\n",
       " 52363: \"'bleedmedry\",\n",
       " 52364: 'pahalniuk',\n",
       " 52365: 'honduras',\n",
       " 10786: 'bursting',\n",
       " 41465: \"pablo's\",\n",
       " 52367: 'irremediably',\n",
       " 40963: 'presages',\n",
       " 57832: 'bowlegged',\n",
       " 65183: 'dalip',\n",
       " 6260: 'entering',\n",
       " 76172: 'newsradio',\n",
       " 54150: 'presaged',\n",
       " 27663: \"giallo's\",\n",
       " 40964: 'bouyant',\n",
       " 52368: 'amerterish',\n",
       " 18523: 'rajni',\n",
       " 30610: 'leeves',\n",
       " 34767: 'macauley',\n",
       " 612: 'seriously',\n",
       " 52369: 'sugercoma',\n",
       " 52370: 'grimstead',\n",
       " 52371: \"'fairy'\",\n",
       " 30611: 'zenda',\n",
       " 52372: \"'twins'\",\n",
       " 17640: 'realisation',\n",
       " 27664: 'highsmith',\n",
       " 7817: 'raunchy',\n",
       " 40965: 'incentives',\n",
       " 52374: 'flatson',\n",
       " 35097: 'snooker',\n",
       " 16829: 'crazies',\n",
       " 14902: 'crazier',\n",
       " 7094: 'grandma',\n",
       " 52375: 'napunsaktha',\n",
       " 30612: 'workmanship',\n",
       " 52376: 'reisner',\n",
       " 61306: \"sanford's\",\n",
       " 52377: '\\x91doña',\n",
       " 6108: 'modest',\n",
       " 19153: \"everything's\",\n",
       " 40966: 'hamer',\n",
       " 52379: \"couldn't'\",\n",
       " 13001: 'quibble',\n",
       " 52380: 'socking',\n",
       " 21931: 'tingler',\n",
       " 52381: 'gutman',\n",
       " 40967: 'lachlan',\n",
       " 52382: 'tableaus',\n",
       " 52383: 'headbanger',\n",
       " 2847: 'spoken',\n",
       " 34768: 'cerebrally',\n",
       " 23490: \"'road\",\n",
       " 21932: 'tableaux',\n",
       " 40968: \"proust's\",\n",
       " 40969: 'periodical',\n",
       " 52385: \"shoveller's\",\n",
       " 25263: 'tamara',\n",
       " 17641: 'affords',\n",
       " 3249: 'concert',\n",
       " 87955: \"yara's\",\n",
       " 52386: 'someome',\n",
       " 8424: 'lingering',\n",
       " 41511: \"abraham's\",\n",
       " 34769: 'beesley',\n",
       " 34770: 'cherbourg',\n",
       " 28624: 'kagan',\n",
       " 9097: 'snatch',\n",
       " 9260: \"miyazaki's\",\n",
       " 25264: 'absorbs',\n",
       " 40970: \"koltai's\",\n",
       " 64027: 'tingled',\n",
       " 19511: 'crossroads',\n",
       " 16121: 'rehab',\n",
       " 52389: 'falworth',\n",
       " 52390: 'sequals',\n",
       " ...}"
      ]
     },
     "execution_count": 16,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reverse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "L_m2OOBpoIVA"
   },
   "source": [
    "- Above output section shows the keys and corrosponding values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8BqTUk_ZoRfk"
   },
   "source": [
    "- Let's get some reviews to know ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "x5QYCFkcI0x4",
    "outputId": "1f98f1c4-bc1a-4370-d023-ba375cdb548a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "getting together some great comedy writers and doing a quality sitcom ' offers another and is fired on the spot # you know good writers cost lots of money ' the big chief # # why we invented reality tv ' br br # could do yet another crime drama ' suggests a man in a bland suit # listening ' the boss replies suddenly interested # like # so let's do another copy of that ' # adds br br # there are already far too many # # out there what can we do to make # stand out ' a naive junior # and is # instantly # out if we do that people may be confused let's give them more of what they already like ' the big chief screams br br # just add more violence and make it really grisly we are fox after all ' another suit suggests to a # # of # you're getting # from the big chief # could make them the # crimes unit ' he goes on to add clearly on a roll br br # # he's got it ' the big chief laughs # the victims could be beautiful and vulnerable women who wear very little on screen ' # that would certainly distract people from the average acting and poor scripts ' # points out br br # it's settled we just need a name ' the big chief # # could call it sex cops violence ' br br # literal how about killer # conveys violence but sounds a bit like basic instinct which had lots of sex ' br br # the big chief replies and then they all slap each other on the back and go and # arrested development\n"
     ]
    }
   ],
   "source": [
    "# Let's see the reviews\n",
    "import keras\n",
    "review = []\n",
    "\n",
    "for i in features[1325]:\n",
    "  review.append(reverse.get(i - 3, \"#\"))\n",
    "print(\" \".join(review))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qwrkhu3TdZ-B"
   },
   "source": [
    "- Another way of reading reviews is shown below.\n",
    "- The paded words are overwriten by PAD and START shows the begining of a sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "g966LUZ08x1h",
    "outputId": "f52b42fe-f038-45f8-fb18-117629dda808"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <START> my personal opinion is that this movie had no real story line to the first john carpenter's vampires but i don't care i loved it jon bon <UNK> derek was great in this movie he really mad me believe that he was the person you would never think he was a famous <UNK> there were some bad things about this movie like the story line there should have been more to the movie there should have been a sequel to the movie that followed the movies story line and they should have kept the same main characters in all three vampires movies i really liked the clothes that the people wore and the setting they pick in mexico i liked how it was old mexico and not new mexico with the clay houses and the old fashion <UNK> i was a little confused with the vampires and how they were able to walk in <UNK> but it was cool how they didn't follow dracula vampire rules\n"
     ]
    }
   ],
   "source": [
    "from keras import utils as np_utils\n",
    "\n",
    "\n",
    "word_to_id = keras.datasets.imdb.get_word_index()\n",
    "\n",
    "word_to_id = {k:(v+3) for k,v in word_to_id.items()}\n",
    "\n",
    "word_to_id[\"<PAD>\"] = 0\n",
    "\n",
    "word_to_id[\"<START>\"] = 1\n",
    "\n",
    "word_to_id[\"<UNK>\"] = 2\n",
    "\n",
    "word_to_id[\"<UNUSED>\"] = 3\n",
    "\n",
    "id_to_word = {value:key for key,value in word_to_id.items()}\n",
    "\n",
    "print(' '.join(id_to_word[id] for id in x_train[3250] ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dybtUgUReCy8"
   },
   "source": [
    "## Build Keras Embedding Layer Model\n",
    "We can think of the Embedding layer as a dicionary that maps a index assigned to a word to a word vector. This layer is very flexible and can be used in a few ways:\n",
    "\n",
    "* The embedding layer can be used at the start of a larger deep learning model. \n",
    "* Also we could load pre-train word embeddings into the embedding layer when we create our model.\n",
    "* Use the embedding layer to train our own word2vec models.\n",
    "\n",
    "The keras embedding layer doesn't require us to onehot encode our words, instead we have to give each word a unqiue intger number as an id. For the imdb dataset we've loaded this has already been done, but if this wasn't the case we could use sklearn [LabelEncoder](http://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.LabelEncoder.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "kj40gnNqukDH",
    "outputId": "c2478b26-b951-4111-b92a-44e6b9350d51"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "A5OLM4eBeCy9"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2hbuTX0MoJsm",
    "outputId": "212a92e9-8d00-4990-c258-c757957eb216"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: gensim in /usr/local/lib/python3.6/dist-packages (3.6.0)\n",
      "Requirement already satisfied: smart-open>=1.2.1 in /usr/local/lib/python3.6/dist-packages (from gensim) (4.1.2)\n",
      "Requirement already satisfied: six>=1.5.0 in /usr/local/lib/python3.6/dist-packages (from gensim) (1.15.0)\n",
      "Requirement already satisfied: numpy>=1.11.3 in /usr/local/lib/python3.6/dist-packages (from gensim) (1.19.5)\n",
      "Requirement already satisfied: scipy>=0.18.1 in /usr/local/lib/python3.6/dist-packages (from gensim) (1.4.1)\n"
     ]
    }
   ],
   "source": [
    "pip install gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "TxNDNhrseCzA"
   },
   "outputs": [],
   "source": [
    "from gensim.test.utils import datapath\n",
    "from gensim.models import KeyedVectors #gensim contains word2ec models and processing tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "yqYM_Vz2qTmJ"
   },
   "outputs": [],
   "source": [
    "glove_file = (\"/content/drive/MyDrive/AIML/NLP Project/glove.6B.100d.txt.gz\") # GloVe file 6B dataset with 100d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "L3CSVVPPeCzD"
   },
   "outputs": [],
   "source": [
    "temp_file = (\"/content/drive/MyDrive/AIML/NLP Project/glove.6B.50d.txt.gz\") # word2vec file 6B dataset with 100d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "rIWfL6Whv5RX"
   },
   "outputs": [],
   "source": [
    "from gensim.scripts.glove2word2vec import glove2word2vec\n",
    "glove2word2vec(glove_file, temp_file) # Converting the Glove file into a Word2vec file\n",
    "model = KeyedVectors.load_word2vec_format(temp_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0AqOnLa2eCzH",
    "outputId": "8e032f5e-d6fb-49cb-d4cc-cf1bac79240d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100,)\n",
      "[ 0.23088    0.28283    0.6318    -0.59411   -0.58599    0.63255\n",
      "  0.24402   -0.14108    0.060815  -0.7898    -0.29102    0.14287\n",
      "  0.72274    0.20428    0.1407     0.98757    0.52533    0.097456\n",
      "  0.8822     0.51221    0.40204    0.21169   -0.013109  -0.71616\n",
      "  0.55387    1.1452    -0.88044   -0.50216   -0.22814    0.023885\n",
      "  0.1072     0.083739   0.55015    0.58479    0.75816    0.45706\n",
      " -0.28001    0.25225    0.68965   -0.60972    0.19578    0.044209\n",
      " -0.31136   -0.68826   -0.22721    0.46185   -0.77162    0.10208\n",
      "  0.55636    0.067417  -0.57207    0.23735    0.4717     0.82765\n",
      " -0.29263   -1.3422    -0.099277   0.28139    0.41604    0.10583\n",
      "  0.62203    0.89496   -0.23446    0.51349    0.99379    1.1846\n",
      " -0.16364    0.20653    0.73854    0.24059   -0.96473    0.13481\n",
      " -0.0072484  0.33016   -0.12365    0.27191   -0.40951    0.021909\n",
      " -0.6069     0.40755    0.19566   -0.41802    0.18636   -0.032652\n",
      " -0.78571   -0.13847    0.044007  -0.084423   0.04911    0.24104\n",
      "  0.45273   -0.18682    0.46182    0.089068  -0.18185   -0.01523\n",
      " -0.7368    -0.14532    0.15104   -0.71493  ]\n"
     ]
    }
   ],
   "source": [
    "# Checking out how the embeddings look like\n",
    "\n",
    "wordEmbed = model['cat']\n",
    "print(wordEmbed.shape)\n",
    "print(wordEmbed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-dUDSg7VeCzM",
    "outputId": "61ba4c4f-ddf8-47ef-cf2a-687520e1c57f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100,)\n",
      "[ 0.30516    0.83637   -0.048112  -0.55369    0.15622    0.67609\n",
      "  0.21536   -0.25701    0.18382   -0.0055563  0.85219   -0.50873\n",
      " -0.72246   -0.23465    0.45078    0.46444   -0.32191   -0.2475\n",
      "  0.011591  -0.10619    0.13508    0.26947   -0.8732    -0.5499\n",
      "  0.040996   1.4222    -0.2778     0.34434    0.64428   -0.39186\n",
      "  0.08322   -0.56255    0.24754    0.48413    0.49154    0.18265\n",
      "  0.30966   -0.20913    0.12835   -0.50545   -0.12319    0.8231\n",
      " -0.054285  -0.4825    -0.10245   -0.30078   -0.32751    0.40399\n",
      " -0.62417    0.24125   -0.56237    0.63873   -0.40588   -0.094381\n",
      " -0.85405    0.58728    0.46708   -0.31234   -0.31759   -0.56132\n",
      " -0.16443   -0.33291   -0.44747    0.014681  -0.15041    0.19614\n",
      " -0.20866   -0.13987   -0.27972   -0.27051    0.17206    0.41843\n",
      " -1.1658     0.14501   -0.1165    -0.23053    0.25514    0.27526\n",
      "  0.78953   -0.061245  -0.20041   -1.3198     0.45839    0.043586\n",
      "  0.22102    0.42491   -0.50759    0.1381    -0.775      0.66423\n",
      "  0.068603   0.19108    0.35848    0.40046    0.22869   -0.60511\n",
      "  0.85939    0.58076   -0.080049   1.0101   ]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "wordEmbed = model['hurray']\n",
    "print(wordEmbed.shape)\n",
    "print(wordEmbed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Tskt_1npeCzP",
    "outputId": "96002a54-23df-4dbb-f828-097fa361fa68"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100,)\n",
      "[-0.18008    0.084106   0.72864   -0.77307   -0.60034   -0.19223\n",
      " -0.22914    0.42666   -0.12717   -0.0065352  0.83655   -0.065268\n",
      " -0.16575   -0.88205    0.1876     0.076791  -0.27967    0.91433\n",
      " -0.39398    0.66955    0.54149    0.32752   -0.094895  -0.51644\n",
      "  0.90136    0.54181    0.26746    0.68016    0.22842   -0.73537\n",
      " -0.20871   -0.059895   0.24477   -0.037463   0.068086  -0.42048\n",
      " -0.94419    0.34877    0.11188   -0.52052    0.53379    0.82981\n",
      "  0.36603   -0.095496  -0.05701    0.61821    0.58893   -0.025414\n",
      "  0.038517  -0.64536   -0.32316   -0.13728    0.73336    0.16069\n",
      "  0.1331    -1.1587     0.82379    0.47982    0.25612   -0.095601\n",
      "  0.21236    0.48335   -0.57491    0.12975    0.3161    -0.078513\n",
      "  0.33765   -0.28684   -0.15185    0.14687    0.68358   -0.30267\n",
      " -0.13771    0.06876    0.31156    0.57659   -0.31029   -0.10508\n",
      "  0.4543    -0.0074264  0.3042    -0.02276    0.017446   0.28472\n",
      " -0.35168   -0.0090702  0.24685   -0.14356   -0.37145   -0.35959\n",
      " -0.17115   -0.069143   0.33538    0.1673     0.3042    -0.49773\n",
      " -0.8218    -0.70383    0.094282   0.80785  ]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "wordEmbed = model['awesome']\n",
    "print(wordEmbed.shape)\n",
    "print(wordEmbed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "EYnjNLLOyDz6",
    "outputId": "a68ddc09-4159-489f-f2d8-3303736dd853"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "awful is in the model\n"
     ]
    }
   ],
   "source": [
    "word = 'awful'\n",
    "if word in model:\n",
    "  print('{0} is in the model'.format(word))\n",
    "else:\n",
    "  print(\"{0} is NOT in the model\".format(word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zAyTUY-eyiGs",
    "outputId": "b30eded4-6900-4f22-c129-622d613bf84d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('horrible', 0.9219714403152466), ('terrible', 0.8742170333862305), ('dreadful', 0.8266334533691406), ('horrendous', 0.7549645900726318), ('scary', 0.7528401017189026), ('unbelievable', 0.7459702491760254), ('ugly', 0.7405971884727478), ('horrifying', 0.7347487211227417), ('sad', 0.7284239530563354), ('weird', 0.709071159362793)]\n"
     ]
    }
   ],
   "source": [
    "print(model.most_similar(positive=['awful']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "JpGxL3Avypp1",
    "outputId": "95c51a1c-0c40-495c-eb3f-556e5ad588ae"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('best', 0.7045565247535706), ('excellent', 0.6884618997573853), ('talent', 0.6607933640480042), ('better', 0.6572389602661133), ('amazing', 0.6427149772644043), ('great', 0.6391830444335938), ('enough', 0.6251134276390076), ('impressive', 0.6189396977424622), ('incredible', 0.6138994693756104), ('perfect', 0.6127669215202332)]\n"
     ]
    }
   ],
   "source": [
    "print(model.most_similar(positive=['awesome', 'good'], negative = ['awful']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "BPblYCEKzRAA",
    "outputId": "ca83e61e-d3e9-4e62-bcd2-6fb80a4a0306"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "excellent\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/gensim/models/keyedvectors.py:895: FutureWarning: arrays to stack must be passed as a \"sequence\" type such as list or tuple. Support for non-sequence iterables such as generators is deprecated as of NumPy 1.16 and will raise an error in the future.\n",
      "  vectors = vstack(self.word_vec(word, use_norm=True) for word in used_words).astype(REAL)\n"
     ]
    }
   ],
   "source": [
    "print(model.doesnt_match(\"awesome awful excellent great man\".split()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3iCrsyV-zgp3",
    "outputId": "2f8cee54-60e6-4d55-ab5f-7c29badff09a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.4280554\n"
     ]
    }
   ],
   "source": [
    "print(model.similarity('male', 'lady'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bsqkvgGDzqey",
    "outputId": "c3d6c162-463c-4c07-8e03-f4b0d49e6b2a"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('excellent', 0.6949935555458069),\n",
       " ('terrific', 0.6836211681365967),\n",
       " ('wonderful', 0.6741393804550171),\n",
       " ('wonderfully', 0.6218067407608032),\n",
       " ('marvelous', 0.594609797000885),\n",
       " ('lovely', 0.5809817314147949),\n",
       " ('admirable', 0.580736517906189),\n",
       " ('unselfish', 0.5805308818817139),\n",
       " ('superb', 0.5724168419837952),\n",
       " ('delightful', 0.5689338445663452)]"
      ]
     },
     "execution_count": 34,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.similar_by_vector(model['excellent'] - model['worst'] + model['awful'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CH8WQpYg0ED5",
    "outputId": "d41577a4-eeab-4e3b-bbe3-1a320b0338db"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('thrilled', 0.9032140374183655), ('pleased', 0.8346080780029297), ('disappointed', 0.8042495250701904), ('impressed', 0.803364634513855), ('surprised', 0.7973885536193848), ('amazed', 0.7768217921257019), ('elated', 0.7742810845375061), ('astonished', 0.7656099796295166), ('excited', 0.7637275457382202), ('overjoyed', 0.7409790754318237)]\n"
     ]
    }
   ],
   "source": [
    "print(model.most_similar(positive = ['delighted']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HDxmN6U6ef2K"
   },
   "source": [
    "-- Building a Sequential model with LSTM, embeddings and drop out layers.\n",
    "- For LSTM we have considered 128 number of neurons with dropout of 0.1\n",
    "- Since it's a two class classification problem, we've considered loss as 'binary crossentropy'. However, the optimizer can be changed based upon the requirement of model accuracy.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "39bt8fFFiDzO",
    "outputId": "a5f648d4-8a40-47fe-8334-f63392bb16df"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        (None, 300, 128)          1280000   \n",
      "_________________________________________________________________\n",
      "lstm (LSTM)                  (None, 128)               131584    \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 100)               12900     \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 101       \n",
      "=================================================================\n",
      "Total params: 1,424,585\n",
      "Trainable params: 1,424,585\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Let's Bulid a Sequential Model with LSTM, Embeddings and drop out layer\n",
    "\n",
    "from keras.models import Sequential, load_model\n",
    "from keras.layers import LSTM, Dense, Dropout, Embedding, Masking, Bidirectional\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "\n",
    "model_seq = Sequential()\n",
    "\n",
    "# Embedding Layer\n",
    "model_seq.add(Embedding(input_dim = vocab_size,\n",
    "                        output_dim = 128, weights = None,\n",
    "                        trainable = True,\n",
    "                        input_length = maxlen))\n",
    "\n",
    "# Recurrent LSTM Layer\n",
    "model_seq.add(LSTM(128, \n",
    "                   return_sequences = False, \n",
    "                   dropout = 0.1, recurrent_dropout = 0.1))\n",
    "\n",
    "# FullyConnected Layer\n",
    "model_seq.add(Dense(100, activation = 'relu'))\n",
    "\n",
    "# Dropout for regularisation\n",
    "model_seq.add(Dropout(0.5))\n",
    "\n",
    "# Output Layer\n",
    "model_seq.add(Dense(1, activation= 'sigmoid'))\n",
    "\n",
    "# Compile the model\n",
    "model_seq.compile(optimizer = 'adam', \n",
    "                  loss = 'binary_crossentropy', \n",
    "                  metrics = ['accuracy'])\n",
    "\n",
    "# Summary of the model\n",
    "model_seq.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RifzZ4UaXz5q"
   },
   "source": [
    "- For each word present, the model looks up the embedding, runs the LSTM one timestep with the embeddings as input and applies the dense layer to generate logits predicting the log-liklihood of the next word.\n",
    "\n",
    "- Since, our output is binary, we need to use sigmoid rather than softmax as activation at last layer and also to evaluate loss used binary_crossentropy rather than categorical_crossentropy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "kf8vpokdXZ29",
    "outputId": "a6dc25c0-50a2-4d7c-93a5-c2630d9bfe44"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "50/50 [==============================] - 24s 422ms/step - loss: 0.6870 - accuracy: 0.5625\n",
      "Epoch 2/3\n",
      "50/50 [==============================] - 21s 423ms/step - loss: 0.4082 - accuracy: 0.8243\n",
      "Epoch 3/3\n",
      "50/50 [==============================] - 21s 423ms/step - loss: 0.2302 - accuracy: 0.9145\n"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "train = model_seq.fit(x_train, y_train, epochs =3, batch_size = 500, verbose = 1 )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "USv-LU_Pf6VA"
   },
   "source": [
    "- With three epochs, the accuracy came to 87%, it can be enhanced by increasing number of epochs. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "id": "3kFsHXNTaI4d"
   },
   "outputs": [],
   "source": [
    "# Let's save the model to file\n",
    "model_seq.save('/content/drive/MyDrive/AIML/NLP Project/model_3epochs.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "BXmyRfLwamHP",
    "outputId": "d80b322f-63ac-443a-f6e2-1f5a1884163b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n"
     ]
    }
   ],
   "source": [
    "# Here, we will load a pretrained model which we have already trained and saved\n",
    "\n",
    "model_seq = load_model('/content/drive/MyDrive/AIML/NLP Project/model_3epochs.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "sR1ywl4Ba-7k",
    "outputId": "d678cd3b-3bf6-4529-9931-4f742620f9c9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "50/50 [==============================] - 22s 427ms/step - loss: 0.1731 - accuracy: 0.9388\n",
      "Epoch 2/5\n",
      "50/50 [==============================] - 21s 425ms/step - loss: 0.1425 - accuracy: 0.9502\n",
      "Epoch 3/5\n",
      "50/50 [==============================] - 21s 427ms/step - loss: 0.1066 - accuracy: 0.9654\n",
      "Epoch 4/5\n",
      "50/50 [==============================] - 21s 426ms/step - loss: 0.0922 - accuracy: 0.9698\n",
      "Epoch 5/5\n",
      "50/50 [==============================] - 21s 424ms/step - loss: 0.0645 - accuracy: 0.9796\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7fb5627c11d0>"
      ]
     },
     "execution_count": 40,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Now the model is loaded, so we will run the command model.fit() to continue training from there.\n",
    "model_seq.fit(x_train, y_train, batch_size = 500, epochs = 5, verbose = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "pif2MAF2bdlX",
    "outputId": "ee0ccb9f-212d-4861-ab0c-272da0bebe4d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "196/196 [==============================] - 13s 64ms/step - loss: 0.0343 - accuracy: 0.9910\n",
      "[0.03430097550153732, 0.9909999966621399]\n",
      "\n",
      " Model Performance: Log Loss and Acuracy on test data are \n",
      "196/196 [==============================] - 13s 64ms/step - loss: 0.5276 - accuracy: 0.8610\n",
      "[0.5275829434394836, 0.8609600067138672]\n"
     ]
    }
   ],
   "source": [
    "print(model_seq.evaluate(x_train, y_train, batch_size = 128))\n",
    "print(\"\\n Model Performance: Log Loss and Acuracy on test data are \")\n",
    "print(model_seq.evaluate(x_test, y_test, batch_size =128))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1fHE9lIidMMA"
   },
   "source": [
    "- The Log Loss and Accuracy for training data are 0.0375 and 0.987 respectively\n",
    "\n",
    "- The Log Loss and Accuracy for testing data are 0.599 and 0.86 respectively.\n",
    "\n",
    "- The model seems to be overfitted.\n",
    "\n",
    "- The model score can be enhanced by hypertuning the parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 320
    },
    "id": "bjIb8WZFkBap",
    "outputId": "2d1a6fc9-7feb-496e-a0f0-8adb7464766b"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x360 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light",
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "y_predict = model_seq.predict(x_test)\n",
    "\n",
    "import seaborn as sns\n",
    "from sklearn import metrics\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "cm = metrics.confusion_matrix(y_test, y_predict.round(), labels = [1,0])\n",
    "df_cm = pd.DataFrame(cm, index = [i for i in ['Actual 1', 'Actual 0']],\n",
    "                     columns = [i for i in ['Predict 1', 'Predict 0']])\n",
    "\n",
    "colormap = plt.cm.viridis  # Color range to be used\n",
    "plt.figure(figsize = (10,5))\n",
    "sns.heatmap(df_cm, annot = True, fmt = 'g', cmap = colormap, linecolor = 'red' );"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "V5_ZOmsUg9hT"
   },
   "source": [
    "- The True positive (TP) value is 10,543, it means out of 25,000 feedbacks, we predicted 10543 feedbacks are positive sentiments and they are actually turned to be positive.\n",
    "- The true negative (TN) value is 10,976, it means out of 25,000 sentiment feedbacks, we predicted 10,976 feedbacks are negative and they all turned out to be negative.\n",
    "- The false positive (FP) value is 1524 out of 25,000 sentiments, it  means we predicted them as positive feedback but actually they turned out to be negative feedback.\n",
    "- The False Negative (FN) value is 1957 out of 25,000 feedbacks, it means we predicted them as negative feedbacks but they turned out to be positive feedback."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "U6dvwwDm4bPY",
    "outputId": "e570edf6-7109-482a-b210-0c2994984437"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.86      0.86      0.86     12500\n",
      "           1       0.86      0.86      0.86     12500\n",
      "\n",
      "    accuracy                           0.86     25000\n",
      "   macro avg       0.86      0.86      0.86     25000\n",
      "weighted avg       0.86      0.86      0.86     25000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(metrics.classification_report(y_test, y_predict.round()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wxY5_vnClcJe"
   },
   "source": [
    "- Not the f1-score value is 86% for both of the positive and negative sentiment feedbacks. (f1 score is the harmonic mean of model's precission and recall)\n",
    "- The model score is also 86%.\n",
    "- The model seems to be good but a little tuning is required to go beyond the 90% accuracy score.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7ASzoofTp3fd"
   },
   "source": [
    "-- let's check the training and validation accuracy to measure the performance of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 453
    },
    "id": "S63QvwZ92dCq",
    "outputId": "67f31f24-2b3a-44de-b295-ec2a701e438b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "40/40 [==============================] - 18s 457ms/step - loss: 0.0864 - accuracy: 0.9700 - val_loss: 0.1901 - val_accuracy: 0.9164\n",
      "Epoch 2/5\n",
      "40/40 [==============================] - 18s 443ms/step - loss: 0.0911 - accuracy: 0.9680 - val_loss: 0.0789 - val_accuracy: 0.9730\n",
      "Epoch 3/5\n",
      "40/40 [==============================] - 18s 446ms/step - loss: 0.0346 - accuracy: 0.9906 - val_loss: 0.0850 - val_accuracy: 0.9708\n",
      "Epoch 4/5\n",
      "40/40 [==============================] - 18s 444ms/step - loss: 0.0189 - accuracy: 0.9961 - val_loss: 0.0974 - val_accuracy: 0.9688\n",
      "Epoch 5/5\n",
      "40/40 [==============================] - 18s 443ms/step - loss: 0.0133 - accuracy: 0.9972 - val_loss: 0.1673 - val_accuracy: 0.9580\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x7fb5617799e8>"
      ]
     },
     "execution_count": 44,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light",
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Running the model with 5 epochs and 500 batch size with a train and validation split of 80% and 20% respectively.\n",
    "history1 = model_seq.fit(x_train, y_train, batch_size = 500, epochs = 5,validation_split = 0.2, verbose = 1)\n",
    "\n",
    "# Assigning the training accuracy to a variable as a DataFrame\n",
    "ta= pd.DataFrame(history1.history['accuracy'])\n",
    "\n",
    "# Assigning the validation accuracy to a variable as a DataFrame\n",
    "va = pd.DataFrame(history1.history['val_accuracy'])\n",
    "\n",
    "# Concatenating both the accuracy and assignin the score to a variable as a DataFrame\n",
    "ta_va = pd.concat([ta, va], axis = 1)\n",
    "\n",
    "# Creating a Box Plot to check the difference between two accuracies.\n",
    "ta_va.boxplot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0bOEj5Qw6Gth"
   },
   "source": [
    "- From the above box plots, it can be observed that there is a small difference between accuracies of training and validation set. \n",
    "- The training accuracy came around 99.8% with a median of 99.5% where as validation accuracy came around 94.8% with almost same median value.\n",
    "- A slight differenec in accuracies can be accomadated by increasing number of neurons in the LSTM layers and can also be overcomed by adding some more epochs.\n",
    "\n",
    "- Now let's visualize the accuracies of each epochs graphically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 280
    },
    "id": "cGyOrxqg6KlT",
    "outputId": "a01e6139-59dc-4a27-e948-c2c9832456fc"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light",
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Visualizing Training and Validation accuracy across the epochs\n",
    "plt.plot(ta, label = 'training')\n",
    "plt.plot(va, label = 'validation')\n",
    "plt.xlabel('epochs')\n",
    "plt.ylabel('accuracy')\n",
    "plt.legend(loc = \"lower right\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4H0PwC7L7Eu4"
   },
   "source": [
    "- With respect to epochs, the training accuracy seems to be increasing initially but becomes stable after 2 epochs and tries to remain above 99%.\n",
    "- However, the validation accuracy seems to be falling down from 99% to 96% with respect to each epochs. This may be due to number of neurons present in the layers.\n",
    "- By hypertuning the parameters, we can make validation accyracy stable and can also bring this one close to the training accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Igq8Qm8GeCzG"
   },
   "source": [
    "## Retrive the output of each layer in keras for a given single test sample from the trained model you built"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AWhb29h1lSJ7"
   },
   "source": [
    "Let's make it more simpler and understandable....."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cQ120HgBjScp",
    "outputId": "756203db-00f9-4da6-e07d-f4b0c6589c2e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 -Layer Output\n",
      "\n",
      "[-0.14466338 -0.13103941  0.02881297 -0.01723955 -0.18364911  0.03636101\n",
      " -0.0794455   0.11615819  0.05335248 -0.04800325  0.05221564 -0.14761288\n",
      "  0.09493496  0.13991147 -0.11863828 -0.08705147  0.14850599  0.13952492\n",
      "  0.01134994  0.07976381  0.21782477  0.00077859 -0.13166803  0.38922\n",
      " -0.10030391  0.06181117 -0.08821342  0.01145477 -0.08212183 -0.06348813\n",
      "  0.13126472  0.14711837  0.05683508  0.11455517 -0.12242979 -0.10675788\n",
      "  0.00321797 -0.236755    0.09138696 -0.08604989  0.06546013 -0.08382159\n",
      " -0.05288065 -0.15182197  0.03179544 -0.00804861 -0.02533646 -0.21650852\n",
      " -0.06853273 -0.00151062 -0.08437602  0.01558676  0.16393591 -0.13319016\n",
      " -0.1367248   0.17785299 -0.0202585  -0.11464594 -0.00127812  0.04673256\n",
      "  0.15422899 -0.02897153 -0.02831157  0.1267308  -0.0702364  -0.18415855\n",
      "  0.09886642  0.24455915  0.07649149  0.21468906  0.07483675  0.13516103\n",
      "  0.09704091  0.0604766   0.14933474  0.13811181  0.01378643  0.06872308\n",
      "  0.1540189   0.06552739  0.09271725  0.05176271  0.06482635  0.27187735\n",
      "  0.18648456  0.09320996 -0.16262726  0.00122158  0.1445955   0.09080635\n",
      "  0.05907442 -0.08790626 -0.05800449 -0.10037402  0.08157134 -0.04361962\n",
      "  0.15576012  0.04185173 -0.08189111  0.02770639 -0.1195543  -0.10181304\n",
      " -0.14223373 -0.1119825  -0.01923428 -0.07164247 -0.10278162 -0.06639259\n",
      " -0.08046363 -0.10082617 -0.12134921 -0.02894369  0.0100637  -0.02359016\n",
      "  0.1106521   0.04176774 -0.1820561   0.03768349  0.1915653  -0.05157375\n",
      "  0.06546237 -0.05035771 -0.16878034  0.02310007  0.13122274  0.19866848\n",
      " -0.16489321  0.12868744] \n",
      " Size: (128,) \n",
      "\n",
      "3 -Layer Output\n",
      "\n",
      "[0.6366689  0.79698914 0.         0.78102386 0.         0.\n",
      " 0.6933264  0.         0.         0.79598457 0.97905123 0.86276996\n",
      " 0.         1.0132754  0.         1.3718534  0.8509364  0.\n",
      " 0.         0.8903509  0.88267326 0.         0.         0.8543534\n",
      " 0.777461   0.         0.88024133 0.         0.8935738  0.\n",
      " 0.7417365  0.762897   0.         0.7730902  0.         0.\n",
      " 1.4491823  0.         0.73400605 0.         0.7531756  0.6510218\n",
      " 0.6568462  0.         0.         1.0683434  0.69357735 0.\n",
      " 0.83443105 0.75878894 0.         0.         0.         0.\n",
      " 0.68120396 0.79183465 0.6246245  0.7576095  0.82820547 0.9843416\n",
      " 0.         0.713689   0.         0.87885994 0.         0.\n",
      " 0.         0.         0.         1.3093417  0.         0.9295136\n",
      " 0.         1.0064797  0.         0.6500282  0.         0.5990715\n",
      " 0.         0.         0.         1.3472757  0.69081175 0.\n",
      " 0.6273223  0.         0.         0.         0.7910961  0.\n",
      " 0.84254843 0.69780487 0.7326348  0.         0.52134633 0.73460466\n",
      " 0.7712671  1.9271752  0.7842317  0.        ] \n",
      " Size: (100,) \n",
      "\n",
      "4 -Layer Output\n",
      "\n",
      "[0.6366689  0.79698914 0.         0.78102386 0.         0.\n",
      " 0.6933264  0.         0.         0.79598457 0.97905123 0.86276996\n",
      " 0.         1.0132754  0.         1.3718534  0.8509364  0.\n",
      " 0.         0.8903509  0.88267326 0.         0.         0.8543534\n",
      " 0.777461   0.         0.88024133 0.         0.8935738  0.\n",
      " 0.7417365  0.762897   0.         0.7730902  0.         0.\n",
      " 1.4491823  0.         0.73400605 0.         0.7531756  0.6510218\n",
      " 0.6568462  0.         0.         1.0683434  0.69357735 0.\n",
      " 0.83443105 0.75878894 0.         0.         0.         0.\n",
      " 0.68120396 0.79183465 0.6246245  0.7576095  0.82820547 0.9843416\n",
      " 0.         0.713689   0.         0.87885994 0.         0.\n",
      " 0.         0.         0.         1.3093417  0.         0.9295136\n",
      " 0.         1.0064797  0.         0.6500282  0.         0.5990715\n",
      " 0.         0.         0.         1.3472757  0.69081175 0.\n",
      " 0.6273223  0.         0.         0.         0.7910961  0.\n",
      " 0.84254843 0.69780487 0.7326348  0.         0.52134633 0.73460466\n",
      " 0.7712671  1.9271752  0.7842317  0.        ] \n",
      " Size: (100,) \n",
      "\n",
      "5 -Layer Output\n",
      "\n",
      "[0.9997334] \n",
      " Size: (1,) \n",
      "\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Model\n",
    "layer_outputs = []\n",
    "for i in range(1, len(model_seq.layers)):\n",
    "    model_temp = Model(model_seq.layers[0].input, model_seq.layers[i].output)\n",
    "    output_temp = model_temp.predict(x_test)[5]\n",
    "    layer_outputs.append(output_temp)\n",
    "    print(i+1, '-Layer Output\\n')\n",
    "    print(output_temp, '\\n', 'Size:', output_temp.shape, \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SIb4ePqhJ_q8"
   },
   "source": [
    "- 0.99147 signifies that 6th number test feedback/point predicted by the model accurately matches the actual feedback given by the user.\n",
    "- The feedback seems to be Positive as per the model.\n",
    "\n",
    "\n",
    "- In this way we can perdict the sentiment of the users and segregate their reviews to enhance the business as per their chioce.    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cUjb77Wqhk9e",
    "outputId": "225e502d-8528-42ba-8cff-ae50fcc570f9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <START> i'm absolutely disgusted this movie isn't being sold all who love this movie should email disney and increase the demand for it they'd eventually have to sell it then i'd buy copies for everybody i know everything and everybody in this movie did a good job and i haven't figured out why disney hasn't put this movie on dvd or on vhs in rental stores at least i haven't seen any copies this is a wicked good movie and should be seen by all the kids in the new generation don't get to see it and i think they should it should at least be put back on the channel this movie doesn't deserve a cheap <UNK> it deserves the real thing i'm them now this movie will be on dvd\n"
     ]
    }
   ],
   "source": [
    "print(' '.join(id_to_word[id] for id in x_test[5] ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "e-XAl1SMk_91"
   },
   "source": [
    "- If we see the review for the movie, it seems to be totally positive and the viewer wants the disney to increase it's popularity and also wants that modern age children should watch it.\n",
    "\n",
    "- It means, the model we have built is solid enough to distinguish the sentiments and predict it correctly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8EOL-sTeEltx"
   },
   "source": [
    "The End\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "SeqNLP_Project1_Questions_Solutions.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
